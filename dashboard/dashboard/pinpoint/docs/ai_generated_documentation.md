This document was generated by running the following colab notebook:

`google3/experimental/users/maximsheshukov/codebase_summarizer/prototype.ipynb`

# Pinpoint: Regression Explorer Technical Documentation

Pinpoint is a Python App Engine service designed for analyzing performance regressions. Its primary objective is to pinpoint the exact commit responsible for a performance regression or improvement, given a metric and a commit range. It achieves this by intelligently navigating potentially complex and noisy performance data, even across thousands of commits and multiple interdependent repositories. Pinpoint's functionality extends beyond just identification; it provides a comprehensive UI for visualizing raw data, investigating root causes, adjusting parameters, and testing potential fixes, thereby empowering users to efficiently diagnose and resolve performance issues.

## Project Terminology

*   **Job**: The top-level unit of work in Pinpoint. A "Job" represents a single request to analyze a performance change, such as a "Bisect" to find a regression commit or a "Try Job" for an A/B comparison. It encapsulates all user-defined parameters, the sequence of operations, and the overall status.
*   **Bisect**: A type of Pinpoint job focused on narrowing down a performance change (regression or improvement) to a specific commit within a given range. It uses a binary search-like algorithm.
*   **Try Job**: A type of Pinpoint job that performs an A/B comparison between two specific code changes (e.g., two Git commits or a commit with a Gerrit patch) to measure the performance difference.
*   **Change**: An immutable representation of a specific state of the source code. A Change can be composed of one or more Git `Commit` objects (for multi-repository projects) and an optional `GerritPatch` (for unmerged code reviews).
*   **Commit**: Represents a single Git commit within a specific repository, identified by its repository URL and Git hash.
*   **GerritPatch**: Represents an unmerged code review on Gerrit, used for testing proposed changes.
*   **Quest**: A declarative definition of a single step or unit of work within a Pinpoint job's workflow (e.g., "Build Chromium," "Run Telemetry Test," "Read Performance Values").
*   **Execution**: A stateful instance of a `Quest` being run for a specific `Change`. It tracks the progress, status, and results of that particular step.
*   **Attempt**: A single, complete run of all `Quests` (e.g., Build, Test, Read Value) for a given `Change`. If any `Execution` within an Attempt fails, the entire Attempt is marked as failed.
*   **Swarming**: An external service (Google's distributed task execution system) used by Pinpoint to run tests and benchmarks on various devices and configurations.
*   **Buildbucket**: An external service (Google's build orchestration system) used by Pinpoint to trigger and monitor builds of source code (e.g., Chromium) to generate testable binaries.
*   **CAS (Content-Addressable Storage)**: An external service (part of RBE, Remote Build Execution) used for storing and retrieving build artifacts and test outputs. Pinpoint uses CAS to manage `isolates`.
*   **Isolate**: A hermetic package of files and commands, typically representing build artifacts or test dependencies, stored on an isolate server or CAS, and used by Swarming to execute tasks.
*   **results2**: A module and API endpoint responsible for generating and serving rich, interactive performance results (often using histograms.html) for a Pinpoint job. It aggregates raw metric data into a user-friendly format.
*   **Sandwich Workflow**: A multi-step automated process initiated by Pinpoint to re-verify an identified culprit by running tests before and after the suspect commit on a specific builder, ensuring the regression is consistently reproducible.
*   **FIFO Scheduler**: A first-in, first-out scheduling mechanism used by Pinpoint to manage the queueing and execution of jobs, often with a "budget" system to prioritize resource allocation.
*   **Execution Engine**: Pinpoint's event-driven, graph-based system (primarily within the `models/task` and `models/evaluators` modules) for defining and progressing complex job workflows in a modular and flexible manner. It uses a `TaskGraph` to represent dependencies.
*   **Web Components**: A set of web platform APIs that allow for the creation of new custom, reusable, encapsulated HTML tags. Pinpoint's frontend (`/elements`) heavily relies on this standard.
*   **Polymer**: A JavaScript library for building Web Components. Pinpoint's UI components are built using Polymer.
*   **FuzzySelect**: A client-side utility (within `/static/autocomplete.js`) that performs fuzzy string matching for autocomplete suggestions, allowing users to find items with partial or slightly misspelled inputs.

## High-Level Overview

Pinpoint addresses the critical need to automate the detection and precise identification of performance regressions (and improvements) in large, continuously evolving software projects like Chromium. Its core functionality is to perform **automated bisection** of code commit ranges against performance metrics.

The project operates as a Python App Engine service, acting as an orchestrator for complex, distributed workflows. It doesn't perform builds or run tests directly; instead, it integrates with external services like **Buildbucket** (for compiling source code and obtaining build artifacts as **isolates**), **Swarming** (for executing performance tests on various target devices and platforms), and **CAS** (for artifact storage and retrieval).

Pinpoint is designed for robustness in challenging environments:
*   **Noisy Metrics**: Employs sophisticated statistical methods (`compare` module) to differentiate real performance changes from inherent measurement variance, often requiring multiple "attempts" (test runs) to build confidence.
*   **Device Affinity**: Gracefully handles scenarios where performance metrics are sensitive to the specific hardware or environment, by ensuring tests are run on consistent configurations.
*   **Flaky/Failing Tests**: Includes mechanisms to retry failed tests and distinguish between infrastructure failures and genuine performance regressions.
*   **Multiple Changes**: Its bisection algorithm is capable of navigating commit ranges with several regressions or improvements, narrowing down to individual culprits.
*   **Large Commit Ranges**: Optimized for efficiency, capable of bisecting ranges spanning thousands of commits by adaptively selecting intermediate points for testing.
*   **Dependent Repositories**: Understands and manages complex `DEPS` file dependencies, allowing it to accurately bisect changes that span across multiple Git repositories (e.g., `chromium/src` and `v8`).

The architecture follows a clear separation of concerns:
*   **Frontend (`/elements`, `/index`, `/static`)**: A Polymer-based Single Page Application (SPA) provides an interactive user interface for configuring jobs, visualizing results, and exploring system statistics.
*   **API/Cron Handlers (`/handlers`, `/dispatcher.py`)**: The entry point for all requests, handling job creation, status updates, cancellations, and periodic background tasks (e.g., scheduling, frozen job recovery).
*   **Core Business Logic and Data Models (`/models`)**: This is the heart of Pinpoint, defining the entities (Jobs, Changes, Quests, Executions), implementing the bisection algorithm, statistical analysis, and orchestration logic for interacting with external services. It manages the state and progression of complex workflows.

## Overall Architecture and Fundamental Concepts

The Pinpoint application is structured into distinct modules, each with a specialized role, communicating through well-defined interfaces.

### Request Handling and Entry Point (`/main.py`, `/dispatcher.py`)

*   **`main.py`**:
    *   **Responsibility**: The absolute entry point for the Pinpoint Python 3 App Engine service. It performs essential bootstrapping, including setting up Google Cloud Logging for observability and optionally enabling the Cloud Debugger for development. It then delegates all HTTP request handling to the `dispatcher` module.
    *   **Design Rationale**: This minimalistic entry point centralizes environment setup (logging, debugging) before the application logic takes over, ensuring a consistent operational foundation.
*   **`dispatcher.py`**:
    *   **Responsibility**: Acts as the central HTTP router for the Pinpoint service. It defines all API endpoints (e.g., `/api/new`, `/api/job/<job_id>`) and internal cron job routes (e.g., `/cron/fifo-scheduler`), mapping incoming requests to specific handler functions within the `/handlers` module. It uses the Flask framework for routing.
    *   **Design Rationale**: A single, comprehensive dispatcher centralizes URL routing logic, making it easy to understand the exposed API surface and internal automation triggers. This separation from the handler implementations keeps individual handler files focused on business logic. The inclusion of `wrap_wsgi_app` ensures App Engine's deferred functionality is available.

### API and Cron Handlers (`/handlers`)

The `/handlers` module forms the primary interface layer for Pinpoint, mediating all client requests and internal automated processes. Each handler is responsible for a specific slice of functionality, ensuring that incoming data is validated, processed, and routed to the appropriate core business logic in the `/models` layer.

*   **`new.py` - Job Creation:**
    *   **Responsibility**: Initiates new Pinpoint jobs. It receives user-defined job parameters via HTTP POST, performs rigorous validation (e.g., `comparison_mode`, `target`, commit ranges, bug IDs), merges these with predefined bot configurations, dynamically constructs the sequence of `Quests` (workflow steps) required for the job, and then schedules the job using the `scheduler` module. It also triggers post-creation actions like updating associated bug reports.
    *   **Design Rationale**: Consolidating job creation ensures consistency in parameter validation and workflow generation. The dynamic `_GenerateQuests` function abstracts the complexity of building a job's execution plan based on user intent, making the system flexible for various test types. Cost-aware scheduling (`scheduler.Schedule`) is a key design choice to manage system resource usage.

*   **`job.py` and `jobs.py` - Job Retrieval and Management:**
    *   **`job.py`**:
        *   **Responsibility**: Retrieves detailed information for a single Pinpoint job given its ID. It also uniquely provides a POST endpoint allowing external systems (like Skia) to "push" a complete job's state into Pinpoint, bypassing the standard scheduling and execution flow. This is crucial for integrating external systems that manage their own job lifecycle but want Pinpoint for visualization, history, and reporting. It uses `MarshalToJob` (and related `MarshalToState`, `MarshalToChange`, `MarshalToAttempt`) to convert external data into Pinpoint's internal model.
        *   **Design Rationale**: Providing both GET (read) and specialized POST (write for external integrations) endpoints for single jobs offers flexibility. The "push" mechanism for external systems enables Pinpoint to act as a universal reporting and visualization platform without dictating the entire workflow for all integrated services.
    *   **`jobs.py`**:
        *   **Responsibility**: Provides an overview of multiple Pinpoint jobs, supporting advanced filtering (by user, configuration, comparison mode, batch ID) and pagination using datastore cursors. It enhances readability by aliasing service account emails to "chromeperf (automation)" in the UI.
        *   **Design Rationale**: Efficiently handling large numbers of jobs requires filtering and pagination to ensure UI responsiveness and prevent overwhelming the client or backend. User-friendly display names for automation accounts improve the user experience.

*   **`cancel.py` - Job Cancellation:**
    *   **Responsibility**: Allows authorized users (job owner or administrator) to terminate queued or running jobs. It enforces strict authorization and, upon successful cancellation, updates the job's internal state to "Cancelled" and posts a comment to any associated bug report.
    *   **Design Rationale**: Secure and transparent job cancellation is vital. Authorization checks prevent unauthorized manipulation. Automatic bug comments ensure all stakeholders are informed of job status changes.

*   **`fifo_scheduler.py` - FIFO Scheduling Cron Job:**
    *   **Responsibility**: A critical cron job that periodically processes Pinpoint's FIFO queues. It iterates through configured queues, selects jobs based on a "budget" (resource cost) and priority, ensuring that only one job for a given configuration is actively running. It transitions jobs from "Queued" to "Running" and triggers their initial tasks.
    *   **Design Rationale**: Centralized, capacity-aware scheduling (`scheduler.GetSchedulerOptions`) prevents resource contention and ensures fair distribution of computational resources. The budget system allows for prioritizing different types of jobs (e.g., fast "try jobs" vs. expensive "bisects") within overall resource limits.

*   **`refresh_jobs.py` - Frozen Job Recovery Cron Job:**
    *   **Responsibility**: A cron job that provides resilience by identifying and attempting to recover "frozen" jobs (those stuck in a "Running" state without updates for an extended period). It employs a retry mechanism and, if a job consistently fails to restart, marks it as ultimately "Failed."
    *   **Design Rationale**: Automatic recovery of frozen jobs is essential for system robustness, preventing jobs from getting permanently stuck due to transient issues or external service outages. Retry limits prevent infinite loops, and metrics track recovery attempts.

*   **`task_updates.py` - Asynchronous Task Progress Updates:**
    *   **Responsibility**: A crucial asynchronous handler that receives real-time status updates from external services (like Swarming or Buildbucket) via Pub/Sub. It decodes these messages, identifies the relevant Pinpoint job and task, and uses the `task_module.Evaluate` mechanism with an `evaluator.ExecutionEngine` to update the job's internal state graph.
    *   **Design Rationale**: An event-driven architecture using Pub/Sub allows Pinpoint to react immediately to external task completions or failures without busy-polling, making the system highly responsive and efficient. This also supports flexible execution models.

*   **`results2.py` - Results Generation and Retrieval:**
    *   **Responsibility**: Manages the generation and retrieval of comprehensive "results2" performance reports for completed Pinpoint jobs. It schedules the asynchronous generation of these potentially complex reports (which involve aggregating and transforming data) to avoid blocking API requests, providing status updates to the client.
    *   **Design Rationale**: Decoupling results generation from immediate API responses ensures UI responsiveness. Storing generated results in Google Cloud Storage and exporting raw data to BigQuery provides scalable storage and powerful analytical capabilities.

### Core Business Logic and Data Models (`/models`)

The `/models` module is the intellectual core of Pinpoint, defining the entities, relationships, and algorithms that drive automated performance analysis. It handles multi-repository code states, orchestrates distributed workflows, performs statistical comparisons, and generates insightful reports.

#### Core Abstractions and Workflow Management

Pinpoint jobs proceed through a hierarchical execution model, breaking down high-level objectives into granular, trackable steps.

*   **`job.py`**:
    *   **Responsibility**: Defines the `Job` Datastore entity, which is the top-level container for all job-related information. This includes user-provided arguments, metadata (creator, creation time), and the overall status. It orchestrates the job's lifecycle, initiating execution, interacting with external systems (e.g., bug trackers, Gerrit), and managing job-level failures.
    *   **Design Rationale**: The `Job` entity serves as the single source of truth for an entire analysis task. Its state machine and methods ensure consistent transitions throughout the job's lifecycle, from creation to completion or failure. For newer jobs, it integrates with the `task` module's execution engine for fine-grained workflow control, while supporting older `JobState` based jobs.

*   **`job_state.py`**:
    *   **Responsibility**: Manages the detailed, dynamic internal state of a Pinpoint `Job`, specifically for jobs using the legacy state management. It holds the lists of `Quests` (steps), `Changes` (code revisions being tested), and `Attempts` (runs of quests on a change). Crucially, it contains the core bisection logic: `Explore()` adaptively identifies new `Changes` to test or requests more `Attempts` based on statistical comparison results. `ScheduleWork()` dispatches pending tasks.
    *   **Design Rationale**: For legacy jobs, this module centralizes the complex, mutable state required for bisection. The `Explore()` method is the intelligent driver of the bisection process, dynamically adjusting the search strategy based on intermediate results. Storing `JobState` as a `PickleProperty` allows for flexible, arbitrary Python object serialization, though it has limitations for schema evolution compared to the newer `task` graph.

*   **`attempt.py`**:
    *   **Responsibility**: Represents a single, full execution of all defined `Quests` for a specific `Change`. It manages a sequence of `Execution` objects (one per Quest), executing them serially. If any `Execution` fails, subsequent `Executions` within that Attempt are skipped, and the entire `Attempt` is marked as failed.
    *   **Design Rationale**: Encapsulates the concept of a single, complete pass over the workflow steps for a given code state. The failure propagation logic simplifies error handling, ensuring that if an early step (like building) fails, subsequent steps (like testing) are not needlessly attempted.

#### Precise Code Identification: The `change` Module

The `/models/change` module provides the foundational abstraction for accurately representing and manipulating specific states of source code, essential for multi-repository bisection.

*   **`change.py`**:
    *   **Responsibility**: Defines the immutable `Change` class, which is the core representation of a code state. A `Change` is a composite of `Commit` objects (one for each relevant repository) and an optional `GerritPatch`. Its `Midpoint()` method is fundamental for bisection, intelligently calculating an intermediate `Change` between two given `Changes`, handling `DEPS` rolls across repositories.
    *   **Design Rationale**: Projects like Chromium are composed of many repositories (`chromium/src`, `v8`, `catapult`, etc.), synchronized by `DEPS` files. A single Git hash is insufficient to define a consistent code state. `Change` provides a unified, immutable view of this multi-repository state, ensuring reproducibility. The `Midpoint()` method is designed to correctly bisect across these complex dependency graphs.

*   **`commit.py`**:
    *   **Responsibility**: Represents a single Git commit within a specific repository. It stores the repository URL and Git hash, resolves Gitiles URLs, and fetches/caches detailed commit metadata (author, message). Its `Deps()` method is critical, parsing the `DEPS` file at that commit to identify all dependent repositories and their pinned revisions.
    *   **Design Rationale**: Centralizes interaction with Gitiles for commit information. The `Deps()` method is key to enabling `Change.Midpoint()` to correctly understand and navigate dependency changes when bisecting across repositories.

*   **`patch.py`**:
    *   **Responsibility**: Encapsulates details about a Gerrit code review patch (server, change ID, revision). It provides `BuildParameters` that integrate with external build systems to build and test unmerged changes.
    *   **Design Rationale**: Allows Pinpoint to test speculative or in-progress code changes before they are merged, which is crucial for verifying fixes or pre-bisections.

*   **`repository.py`**:
    *   **Responsibility**: Provides a mapping from short, user-friendly repository names (e.g., 'chromium') to their canonical Gitiles URLs.
    *   **Design Rationale**: Abstracts away specific Gitiles URLs from the rest of the application, making configurations more manageable and resilient to URL changes.

*   **`commit_cache.py`**:
    *   **Responsibility**: A lightweight, memcache-only component for storing frequently accessed but expensive-to-fetch metadata for `Commit` and `GerritPatch` objects.
    *   **Design Rationale**: Reduces latency and API call overhead to external services like Gitiles and Gerrit by caching transient data in memcache, optimizing performance for common lookups without incurring Datastore write costs.

**Key Workflow: Bisecting a Change Range**

```
Change A (start of range)                        Change B (end of range)
        |                                               |
        V                                               V
[change.Change.Midpoint(Change A, Change B)]
        |
        +-- Check for patch compatibility; if incompatible, Raise NonLinearError
        |
        +-- Call _ExpandDepsToMatchRepositories(A.commits, B.commits)
        |   (Purpose: Ensure commit lists include all relevant repos by parsing DEPS)
        |       |
        |       +-- For each Commit: commit.Commit.Deps()
        |       |   (Fetches DEPS file from Gitiles, parses via gclient_eval)
        |       V
        |   Mutates commit lists in-place to align all dependencies found
        |
        +-- Call _FindMidpoints(aligned_commits_A, aligned_commits_B)
        |       |
        |       +-- For each (commit_a, commit_b) pair in aligned lists:
        |       |       commit.Commit.Midpoint(commit_a, commit_b)
        |       |             |
        |       |             V
        |       |       gitiles_service.CommitRange(repo_url, hash_a, hash_b)
        |       |           (Finds midpoint within a single repo's linear history)
        |       |
        |       +-- If adjacent commits have DEPS changes:
        |       |       Re-evaluate and expand dependency lists to cover new repos
        |       V
        |   Returns list of midpoint Commits (one for each repository)
        |
        V
New Change Object (representing the midpoint, with original patch if any)
```

#### Flexible Work Execution: The `task` and `evaluators` Modules

These modules define Pinpoint's modern, event-driven execution engine, providing a highly flexible and scalable way to manage complex, dynamic job workflows.

*   **`task.py`**:
    *   **Responsibility**: Defines the core concepts for the execution engine: `Task` (a Datastore entity representing a unit of work), `TaskVertex`, `Dependency`, and `TaskGraph` (namedtuples defining the graph structure). Its central method, `Evaluate()`, performs a depth-first traversal of the `TaskGraph`, calling specialized `evaluator` functions for each task, processing `Event` objects to trigger state changes, and dynamically extending the graph as needed.
    *   **Design Rationale**: This explicit task graph and event-driven model offers superior flexibility, scalability, and clarity compared to the older `PickleProperty`-based `JobState`. It decouples task logic from graph traversal, enabling dynamic workflow changes, modular evaluators, and robust error handling. Tasks have strict state transition rules to ensure data consistency.

*   **`event.py`**:
    *   **Responsibility**: Defines the `Event` namedtuple, which acts as the canonical input for `task.Evaluate()`. Events (e.g., `'initiate'`, `'update'`, `'select'`) signal internal state changes or external stimuli, driving the progression of the task graph by triggering evaluators.
    *   **Design Rationale**: Centralizing event definitions provides a clear, standardized interface for triggering actions within the task graph. This event-driven approach makes the execution engine reactive and easy to extend.

*   **`/models/evaluators`**:
    *   **Responsibility**: This package provides a highly composable framework for defining the "how-to" logic for `task.Evaluate()`. It consists of small, reusable `Filters` (e.g., `TaskTypeEq`) and `Evaluators` (e.g., `TaskPayloadLiftingEvaluator`, `SequenceEvaluator`, `FilteringEvaluator`) that can be combined declaratively. Key components like `DispatchByTaskType` and `DispatchByEventTypeEvaluator` dynamically route tasks and events to the correct, specialized evaluation logic.
    *   **Design Rationale**: The composable evaluator pattern avoids monolithic conditional logic, making the system easier to understand, test, maintain, and extend. By clearly separating filtering (what to evaluate) from evaluation (how to evaluate), it promotes modularity and reusability, particularly for complex state transitions in the `TaskGraph`.

*   **`evaluators/job_serializer.py`**:
    *   **Responsibility**: A specialized `Evaluator` that transforms the granular internal `TaskGraph` state into a structured, human-readable dictionary. This serialized data is suitable for consumption by external clients, like the Pinpoint Web UI, presenting a high-level view of the job's progress, parameters, and results.
    *   **Design Rationale**: The internal `TaskGraph` is detailed and highly structured for execution, but too verbose for direct UI display. This serializer acts as a presentation layer, aggregating, reordering, and formatting data to make it digestible and actionable for users, without exposing internal engine complexities.

**Key Workflow: Job Serialization (from `evaluators/job_serializer.py`)**

```
+----------------+       +------------------------------------+
| Incoming Task  | ----> | job_serializer.Serializer.__call__ |
|  & Event       |       +------------------------------------+
|  (e.g., from    |                 | (Delegates based on Task Type)
|  task.Evaluate)|                 |
+----------------+                 v
                                   +-----------------------------------------------------+
                                   | DispatchByTaskType (Routes to specific Evaluators)  |
                                   +-----------------------------------------------------+
                                     |
    +--------------------------------+---------------------------------+
    |                                |                                 |
    v                                v                                 v
+--------------------------+  +-------------------------------+  +--------------------------+
| SequenceEvaluator for    |  | performance_bisection.        |  | Other task-specific      |
| 'find_isolate',          |  | Serializer for 'find_culprit' |  | Serializers              |
| 'run_test', 'read_value' |  | tasks                         |  |                          |
| tasks                    |  |                               |  |                          |
+--------------------------+  +-------------------------------+  +--------------------------+
       |                         |                                  |
       v                         v                                  v
+------------------------+  +------------------------+  +------------------------+
| Task-specific Serializer|  | AnalysisTransformer   |  | (e.g., find_isolate.S)|
| (e.g., run_test.S)     |  | (transforms raw analysis|  |                        |
|                        |  | data into results)     |  |                        |
+------------------------+  +------------------------+  +------------------------+
       |                         |                                  |
       v                         v                                  v
+------------------------+  +--------------------------+  +------------------------+
| TaskPayloadLiftingEvaluator |  | Local Context for Culprit|  | TaskTransformer   |
| (extracts relevant       |  |  Data                    |  | (e.g., formats links)|
| payload fields)          |  |                          |  |                        |
+------------------------+  +--------------------------+  +------------------------+
       |                         |                                  |
       |  (Returned to DispatchByTaskType)
       +-------------------------------------------------------------+
                                   | (DispatchByTaskType merges local contexts into global)
                                   v
+----------------------------------------------------------------------------------+
| Global Job Context (Accumulator)                                                 |
| (Updated with aggregated task states, ordered changes, comparisons, and         |
| high-level job parameters for UI display)                                        |
+----------------------------------------------------------------------------------+
```

#### Intelligent Search for Performance Changes: Statistical Comparison and Exploration

These modules provide Pinpoint's analytical capabilities, enabling it to detect meaningful performance shifts and efficiently pinpoint their root causes.

*   **`/models/compare`**:
    *   **Responsibility**: Offers a robust statistical framework for comparing two sets of performance metric samples (e.g., baseline vs. candidate commit). It produces one of three decisions: `DIFFERENT`, `SAME`, or `UNKNOWN`. It leverages both the Kolmogorov-Smirnov (K-S) and Mann-Whitney U (MWU) statistical tests.
    *   **Design Rationale**: Performance data is inherently noisy. Relying on a single statistical test or a binary "different/not different" decision is insufficient. By using both K-S (sensitive to distribution shape) and MWU (sensitive to central tendency), and taking the minimum p-value, it captures more nuances. The three-way decision (`DIFFERENT`, `SAME`, `UNKNOWN`) is crucial: `UNKNOWN` prevents premature conclusions and guides the system to collect more data (`Attempts`), improving statistical confidence and preventing false positives or negatives.
    *   **`kolmogorov_smirnov.py`, `mann_whitney_u.py`**: Provide direct Python implementations of these statistical tests.
    *   **`thresholds.py`**: Manages dynamic significance thresholds (`low_threshold`, `high_threshold`) that adapt based on comparison `mode`, expected `magnitude` of change, and `sample_size`. These thresholds are pre-calibrated through extensive simulations to ensure statistical rigor in the `UNKNOWN` decision zone.

*   **`exploration.py`**:
    *   **Responsibility**: Implements the `Speculate()` function, which is central to Pinpoint's binary search algorithm for finding culprits. Given a range of `Changes` and a function to detect differences, `Speculate()` performs a binary infix traversal, intelligently proposing new intermediate `Changes` for further testing.
    *   **Design Rationale**: Automating bisection over a large commit range requires an efficient search strategy. `Speculate()` enables looking "levels ahead" in the bisection tree by proposing multiple intermediate points for concurrent testing. This trades increased parallelization (and thus compute resources) for a significantly reduced overall time-to-culprit, which is critical for quick diagnosis in CI/CD pipelines.

#### Orchestrating External Services: Concrete `Quest` Implementations

The `/models/quest` module defines the abstract and concrete units of work that Pinpoint executes, enabling modular interaction with diverse external services like Buildbucket and Swarming.

*   **`quest.py`**:
    *   **Responsibility**: Defines the abstract `Quest` base class. A `Quest` is a declarative definition of a single work unit (e.g., "Build Chromium"). It provides a `Start()` method to initiate an `Execution` and `PropagateJob()` for context. It also includes a factory method `FromDict()` for creating `Quest` instances from configuration dictionaries.
    *   **Design Rationale**: This abstraction allows for modular definition of workflow steps. Each specific `Quest` (e.g., `FindIsolate`, `RunTelemetryTest`) can encapsulate its own logic for interacting with external services, keeping the overall workflow definition clean.

*   **`execution.py`**:
    *   **Responsibility**: Defines the abstract `Execution` base class. An `Execution` is a stateful instance of a `Quest` actively running for a specific `Change`. It tracks its `_completed` status, `_exception` details (if any), `_result_values` (extracted metrics), and `_result_arguments` (data to be passed to subsequent executions). Its `Poll()` method is periodically called to advance its state, including robust error handling to differentiate fatal job errors from recoverable execution failures.
    *   **Design Rationale**: `Execution` objects are the granular tracking units of work. Their stateful nature allows Pinpoint to monitor long-running external tasks (like builds or Swarming jobs). The explicit error handling mechanism (`JobError` vs. execution-specific errors) allows for flexible recovery strategies, preventing minor failures from halting an entire job unnecessarily.

*   **`find_isolate.py`**:
    *   **Responsibility**: Implements the `FindIsolate` `Quest` and its `_FindIsolateExecution`. This quest obtains a build artifact (an isolate hash or CAS root reference) for a given `Change`.
    *   **How it works**: It first checks an internal cache for a pre-existing artifact. If not found, it orchestrates a build using Buildbucket. The `_FindIsolateExecution` then polls Buildbucket for the build status, extracts the artifact reference upon success, and passes this reference as `result_arguments` to the subsequent quest (e.g., `run_test`). This decouples artifact retrieval from test execution.

*   **`run_test.py`**:
    *   **Responsibility**: Serves as the base `Quest` for running any command or test on Swarming bots. It handles the generic logic for constructing Swarming task requests (dimensions, command, tags), scheduling these tasks, polling their status, and interpreting the outcomes.
    *   **How it works**: It takes isolate/CAS references provided by preceding quests (e.g., `FindIsolate`). Its `_RunTestExecution` monitors Swarming task states (`PENDING`, `RUNNING`, `COMPLETED`, `EXPIRED`, `FAILURE`), extracts any output isolate/CAS root references, and reports Swarming job metrics. This module abstracts the complexities of direct Swarming API interaction.

*   **`read_value.py`**:
    *   **Responsibility**: Implements the `ReadValue` `Quest` and its `ReadValueExecution`. Its core task is to extract specific numerical performance metrics from raw output files (e.g., `perf_results.json`) generated by completed test runs.
    *   **How it works**: It retrieves the specified output file from an isolate server or CAS using the reference from the previous quest (`run_test`). It then parses the file, supporting formats like legacy GraphJSON and modern HistogramSet, extracting the target metrics based on user-defined filters (metric name, story, statistic). It also handles cases of missing or malformed data by raising specific `ReadValue` errors. The extracted values become `_result_values`.

*   **Specialized `run_*.py` Quests (e.g., `run_browser_test.py`, `run_telemetry_test.py`):**
    *   **Responsibility**: These modules extend `run_test.py` to provide specific implementations tailored for different testing frameworks and environments within Chromium and related projects.
    *   **How it works**: They override base methods like `_ComputeCommand` and `_ExtraTestArgs` to generate the precise command lines, arguments, and environment variables needed to execute specific test types (e.g., Telemetry benchmarks, GTests) on Swarming. This abstracts away the intricate details of invoking each test system.

**Key Workflow: General Pinpoint Job Flow (Quest and Execution Lifecycle)**

```
                  Job Starts (Job object transitions to Running)
                     |
                     V
          +--------------------+
          |   Quest 1 (Build)  |  (e.g., FindIsolate Quest instance)
          +--------------------+
                | Start(change_object)
                V
          +--------------------+    _Poll() is called periodically by Pinpoint backend
          | Execution 1 (Build)| <-------------------------------+
          | - Submits Buildbucket job |                            |
          | - Monitors Buildbucket status until completion        |
          +--------------------+                                   |
                | _Complete()                                     |
                | (result_arguments: isolate_hash or cas_root_ref for build output)
                v                                                 |
          +--------------------+                                 |
          |   Quest 2 (Test)   |  (e.g., RunTelemetryTest Quest instance)
          +--------------------+                                 |
                | Start(change_object, isolate_hash/cas_root_ref from prev. execution) |
                V                                                 |
          +--------------------+                                 |
          | Execution 2 (Test) | <-------------------------------+
          | - Submits Swarming task |                              |
          | - Polls Swarming status until completion               |
          +--------------------+                                   |
                | _Complete()                                     |
                | (result_arguments: cas_root_ref for test output logs/histograms)
                v                                                 |
          +--------------------+                                 |
          |  Quest 3 (Read Value)| (e.g., ReadValue Quest instance)
          +--------------------+                                 |
                | Start(change_object, cas_root_ref from prev. execution) |
                V                                                 |
          +--------------------+                                 |
          | Execution 3 (Read Value)| <---------------------------+
          | - Retrieves output file from CAS/Isolate server       |
          | - Parses metrics (e.g., from perf_results.json)       |
          +--------------------+
                | _Complete()
                | (result_values: [extracted_metric_value])
                v
                  Job Finished (Job object transitions to Completed/Failed)
```

#### Automated Culprit Identification: `performance_bisection.py`

This module contains the core algorithm for Pinpoint's automated performance bisection, dynamically constructing and refining the task graph to find the responsible commit(s).

*   **Responsibility**: Implements the `find_culprit` task. It dynamically manages the bisection process by:
    1.  Fetching all commits in the specified range.
    2.  Initiating `read_value` tasks for range boundaries and dynamically generated intermediate commits.
    3.  Processing `read_value` task results using `compare.Compare` for statistical analysis.
    4.  Employing `exploration.Speculate` to determine the next steps:
        *   Identifying a `culprit` if a significant performance difference is localized to a small range.
        *   Increasing `Attempts` for existing `read_value` tasks if statistical comparisons are `UNKNOWN`.
        *   Adding new `read_value` tasks for intermediate commits to further narrow the search space.
*   **Design Rationale**: Automates the often time-consuming and manual process of bisecting performance regressions. By combining commit range management, statistical comparison, and an adaptive search (`exploration.Speculate`), it efficiently finds culprits, even in large ranges, minimizing human effort. The iterative, dynamic nature of the task graph (`RefineExplorationAction`) allows the system to gather more data or refine the search as needed.

#### Job Lifecycle Management and Reporting

This collection of modules ensures Pinpoint jobs are efficiently scheduled, their progress and outcomes are clearly communicated, and their historical data is preserved for future analysis.

*   **`scheduler.py`**:
    *   **Responsibility**: Implements a simple FIFO (First-In, First-Out) job scheduler. Jobs are organized into `ConfigurationQueue` instances (one per builder configuration). `PickJobs()` selects jobs for execution based on priority, submission time, and a configurable `budget` (resource limit). It also tracks queueing delays.
    *   **Design Rationale**: A transparent and manageable queueing system is essential for shared resources. The budget system provides a mechanism for fair scheduling and prioritizing different job types, ensuring the system remains responsive while handling varying loads.

*   **`job_bug_update.py`**:
    *   **Responsibility**: Formats and posts structured updates to external issue trackers (e.g., Monorail) in response to significant job events, such as job creation, finding differences, or missing values. It uses Jinja2 templates for consistent formatting.
    *   **Design Rationale**: Automates communication with issue trackers, reducing manual overhead and ensuring consistent, information-rich updates. It can dynamically assign owners (e.g., to the culprit's author), apply relevant labels, and even merge duplicate issues, streamlining the resolution process.

*   **`results2.py`**:
    *   **Responsibility**: Manages the generation and storage of user-facing job results, typically as an interactive `results2.html` page. It renders collected histograms into a `vulcanized_histograms_viewer.html` template and stores the result in Google Cloud Storage. It also exports detailed job and performance metric data to BigQuery for long-term analytics.
    *   **Design Rationale**: Provides a comprehensive and interactive view of job results, making complex performance data accessible to users. Leveraging Google Cloud Storage for static assets and BigQuery for raw data ensures scalability, durability, and robust capabilities for deep analysis and dashboarding.

*   **`timing_record.py`**:
    *   **Responsibility**: Records historical execution times for completed Pinpoint jobs and provides runtime estimates for new jobs. It stores `TimingRecord` entities with job duration and associated tags.
    *   **Design Rationale**: Provides valuable operational insight into job performance. By using historical data, it can offer users realistic estimated completion times for new jobs, improving user experience and planning.

*   **`sandwich_workflow_group.py`**:
    *   **Responsibility**: Manages the state and progress of automated culprit verification workflows, also known as "sandwich" builds. It initiates external `CloudWorkflow` executions to perform re-verification steps and tracks their completion. Once all workflows are done, it summarizes the findings and updates the associated bug in the issue tracker.
    *   **Design Rationale**: Automates the crucial step of re-verifying identified culprits to reduce false positives. By integrating with Cloud Workflows, it enables complex, multi-step verification processes and automatically updates issue trackers based on the outcome, streamlining the resolution of regressions.

#### Error Handling

*   **`errors.py`**:
    *   **Responsibility**: Defines a comprehensive hierarchy of custom exception classes (`JobError`, `FatalError`, `InformationalError`, `RecoverableError`).
    *   **Design Rationale**: A structured approach to error handling allows Pinpoint to categorize failures precisely (e.g., infrastructure build errors, test failures, Pinpoint internal logic errors, user input issues). This enables the system to respond appropriately—halting fatal errors, retrying recoverable ones—and provides clearer, more actionable feedback to users and for internal debugging.

### User Interface (`/elements`, `/index`, `/static`)

These modules constitute the Pinpoint web application's user interface layer, providing interactive tools for job creation, monitoring, and result analysis.

#### Application Bootstrap (`/index`)

*   **`/index/index.html`**:
    *   **Responsibility**: The application's main entry point, loaded by the browser. It sets up the foundational environment by importing all necessary external libraries (Polymer, D3.js, Google Sign-In client) and custom Web Components (`index-page.html`). It also defines basic page styling and title.
    *   **Design Rationale**: By importing all dependencies centrally, it ensures that the application's core technologies are available before any specific application logic or UI is rendered. Its minimalistic `<body>` content, containing only `<index-page>`, highlights the application's component-driven design where a single root component builds the entire UI.

*   **`/elements/index-page.html`**:
    *   **Responsibility**: As a custom Polymer element, `index-page.html` serves as the main application container and orchestrator. It encapsulates primary application logic, manages the overall layout, integrates the `navigation-bar`, and dynamically renders different views (pages) based on client-side routing. It handles global application state, including user authentication and API authorization headers.
    *   **Design Rationale**: Its central role as the sole element in the `index.html` body underscores the application's commitment to modularity. It aggregates various sub-components to form the complete application interface, abstracting the complexity of the application structure from the root HTML file and enabling a Single Page Application (SPA) experience.

*   **`/elements/navigation-bar.html`**:
    *   **Responsibility**: Provides consistent navigation across the application. It includes the application's logo, Google Sign-In controls for user authentication, links to different features (e.g., jobs list, documentation), and a bug reporting mechanism that pre-populates reports with current page context.
    *   **Design Rationale**: Separating navigation into a reusable component ensures a consistent user experience. Integrating Google Sign-In leverages an established identity platform for secure and easy user management. Automatic bug report population streamlines user support.

**Application Initialization Workflow**

```
User navigates to Pinpoint URL
       |
       v
Browser loads `/index/index.html`
       |
       +--- Fetches Web Components Polyfills & Polymer framework (base for UI)
       |
       +--- Fetches D3.js library (for data visualization)
       |
       +--- Fetches Google Sign-In client library (for authentication)
       |
       +--- Imports custom Polymer elements (e.g., index-page.html, navigation-bar.html)
       |
       v
`<index-page>` custom element is instantiated in the DOM
       |
       v
`<index-page>` constructs the main application layout:
  - Initializes client-side routing (`app-location`, `app-route`, `iron-pages`).
  - Integrates `<navigation-bar>` for global navigation and authentication.
  - Renders the initial view (e.g., jobs list) based on the current URL path.
       |
       v
Application UI is fully rendered and interactive, responding to user actions and URL changes.
```

#### Core UI Elements (`/elements`)

The `/elements` module is the core UI layer, using Polymer to build a collection of reusable Web Components. It translates backend data into interactive, understandable, and actionable information for users.

*   **Design Philosophy**: Driven by modularity, maintainability, and user-centric design.
    *   **Component-Based Architecture (Polymer)**: Polymer and Web Components ensure strong encapsulation of UI logic, styling, and behavior, simplifying development, testing, and reuse.
    *   **Consistent Styling (`base-style.html`)**: Defines a unified visual language, CSS variables for z-index management, and common styles, guaranteeing aesthetic cohesion.
    *   **Standardized Data Loading (`loading-wrapper.html`)**: Centralizes patterns for indicating loading states and displaying errors during asynchronous data fetching, ensuring consistent user feedback.
    *   **Enhanced User Input (`autocomplete-box.html`)**: Improves usability for complex input fields (e.g., bot names, commit hashes) by providing real-time, fuzzy suggestions.
    *   **Client-Side Routing (`index-page.html`)**: Creates a Single Page Application (SPA) experience, offering fluid navigation without full page reloads.
    *   **Contextual Feedback (`paper-toast`, `paper-dialog`)**: Provides both ephemeral and modal feedback mechanisms for various user interactions and confirmations.
    *   **Integration with External Services (`navigation-bar.html`, `job-page.html`)**: Links directly to source code, bug trackers, and analysis tools, positioning Pinpoint as a central hub without reimplementing external functionalities.

*   **Key Components:**
    *   **`base-style.html`**: Establishes global CSS variables (e.g., `--layer-menus` for z-index management), typography, and common styling patterns for consistent visual identity.
    *   **`loading-wrapper.html`**: A utility that wraps content, displaying a `paper-spinner` during asynchronous data fetching and presenting error messages upon failure, centralizing visual feedback.
    *   **`autocomplete-box.html`**: Provides a robust input field with fuzzy auto-completion, suggestions, grouping, and tag support, enhancing data entry accuracy and speed.
    *   **`error-page.html`**: A fallback component displayed for non-existent routes, providing a clear "Page not found" message.
    *   **`jobs-page` (sub-module)**: Manages *creating and listing* performance analysis jobs.
        *   **Responsibilities**: Provides the UI for initiating, configuring, and monitoring jobs (Try Jobs and Bisects). Emphasizes dynamic forms, real-time data fetching, and user-friendly commit selection.
        *   **Key Functionality**: Includes `attempt-count-help.html` (iteration count info), `commit-input.html` (commit autocompletion), `commit-details.html` (commit metadata display), `jobs-table.html` (sortable job list), and `new-job-fab.html` (core new job configuration and submission).
    *   **`job-page` (sub-module)**: Designed for *in-depth viewing and analysis of a single job*.
        *   **Responsibilities**: Presents detailed job information, results, and troubleshooting aids, including local reproduction commands. Acts as a comprehensive dashboard for a specific job.
        *   **Key Functionality**: Includes `change-details.html` (commit specifics, test attempts, local reproduction commands), `change-info.html` (summary commit info), `exception-details.html` (detailed error messages), `execution-details.html` (fine-grained task details, Swarming reproduce commands), `job-chart.html` (D3.js visualization of results over time), `job-details.html` (overall job configuration and links), `job-menu-fab.html` (job-related actions).
    *   **`cancel-job-dialog.html`**: A modal dialog for cancelling jobs, requiring a reason.
    *   **`results2-frame.html`**: An iframe wrapper for embedding external "Results2" pages, handling loading status and compatibility warnings.
    *   **`queue-stats-page.html`**: Provides real-time statistics for specific job queues (queued/running jobs, waiting time histograms), offering transparency into resource utilization.
    *   **`stats-page.html`**: Displays aggregate operational metrics for the entire system (job reliability, latency, duration histograms) over various timeframes.
    *   **`migrate-page.html`**: An administrative page for initiating and monitoring backend data migration processes, showing progress and errors.

**Key Workflow: Initiating and Configuring a New Job (detailed in `/elements/jobs-page`)**

```
User Clicks "Add" (or "Refresh" to restart job)
      |
      V
`new-job-fab` opens the job creation dialog
      |
      V
`new-job-fab` fetches available Bots and Benchmarks from backend APIs
      |
      V
User selects Bot ("configuration") and Benchmark
      |
      V
`new-job-fab` fetches detailed Benchmark Configuration (stories, metrics)
      |
      V
User selects Job Type: "Try Job" (A/B) or "Bisect" (regression)
      |
      V
If "Try Job": User specifies Base and Experiment Git Hashes/Builds and optional patches/args
      |
      V
If "Bisect": User specifies Start and End Git Hashes for the bisect range, and the metric to track
      |
      V
(Throughout commit selection, `commit-details` fetches and displays commit info)
      |
      V
User fills in other details (Story, Story Tags, Project, Bug ID, Batch ID, Extra Test Args)
      |
      V
User Clicks "Start"
      |
      V
`new-job-fab` validates inputs and submits form data to the backend `/api/new`
      |
      V
On successful job creation: `app-location` redirects user to `/job/<jobId>`
      |
      V
On submission error: `new-job-fab` displays error message within the dialog
```

**Key Workflow: Exploring Job Performance Over Time (detailed in `/elements/job-page`)**

```
User navigates to a job page
       |
       v
[job-details] - Shows overall job arguments, links to bug/analysis tools.
       |
       v
[job-chart]   - Visualizes performance metric across multiple commits using D3.js.
                Highlights detected differences.
                Allows user to select a specific commit for detail.
       |
       | (User selects a commit)
       v
[change-details] - Displays details for the selected commit:
                     - Commit info (`change-info`).
                     - Grid of attempts/executions status (e.g., build success/failure, test status).
                     - Prompts user to select an attempt/execution for further details.
       |
       | (User selects an attempt/execution)
       v
[execution-details] - Shows specific metrics and metadata for that execution.
                     - If failed, integrates [exception-details] for traceback.
                     - Provides "swarming reproduce" command for local debugging.
```

#### Client-Side Assets and Testing (`/static`)

The `/static` module centralizes front-end assets, core JavaScript utilities, and dedicated testing infrastructure for client-side components.

*   **`autocomplete.html` (containing `autocomplete.js`)**:
    *   **Responsibility**: Implements a robust fuzzy autocomplete mechanism for enhancing user input. The `FuzzySelect` class performs fuzzy matching and scoring, while the `Autocomplete` class integrates this for dynamic suggestions, grouping, and structured result sorting.
    *   **Design Rationale**: Fuzzy matching significantly improves user experience for fields with many options by tolerating partial or slightly misspelled inputs, reducing cognitive load and accelerating data entry. The scoring mechanism ensures relevant suggestions are prioritized.

*   **Visual Assets (`logo.png`, `logo.svg`, `sort-down.svg`, etc.)**:
    *   **Responsibility**: Provides branding logos (raster and vector) and standardized iconography for UI controls, such as sorting indicators in tables.
    *   **Design Rationale**: Multiple logo formats ensure broad compatibility and scalability. Clear visual cues for sorting states improve UI intuition and usability.

*   **`testing_common.html` (containing `testing_common.js`)**:
    *   **Responsibility**: Provides a suite of utilities primarily for mocking network requests (`XMLHttpRequest`) and managing DOM elements in client-side unit and integration tests.
    *   **Design Rationale**: Testing UI components that rely on HTTP requests requires isolation from actual network calls to ensure fast, deterministic, and self-contained tests. Mocking `XMLHttpRequest` with `addXhrMock` achieves this. Fixture management (`addToFixture`, `clearFixture`) provides a controlled, clean DOM environment for each test. Utility functions for URL parameter handling ensure consistent URL matching in mocks.

**Workflow for XHR Mocking (in `testing_common.js`)**

```
Test Setup:
[Test Script] ------------> addXhrMock(url_pattern, mock_response_data)
      |                       |
      |                       V
      |                  [testing_common.js]
      |                       |  - Stores url_pattern-response mapping.
      |                       |  - Replaces global window.XMLHttpRequest with MockXMLHttpRequest.
      |                       V

During Test Execution (Application Code under test):
[Application Component] --> new XMLHttpRequest().open(method, url).send(data)
      |                       |
      |                       V
      |                  [MockXMLHttpRequest] (intercepts the call)
      |                       |  - Intercepts open(method, url) and send(data).
      |                       |  - Matches requested `url` against stored `url_pattern`s
      |                       |    (prioritizing exact matches, then regex, then wildcard).
      |                       V
      |                  [testing_common.js]
      |                       |  - Retrieves the associated `mock_response_data`.
      |                       V
      |                  [MockXMLHttpRequest]
      |                       |  - Sets `this.responseText` to `mock_response_data`.
      |                       |  - Calls `this.onload()` (simulating network completion and success).
      V                       |
[Application Component] <---- Receives Mock Response Data (responseText), proceeds as if from network
      |                       |

Test Teardown:
[Test Script] ------------> clearXhrMock()
      |                       |
      |                       V
      |                  [testing_common.js]
      |                       |  - Restores original window.XMLHttpRequest.
      |                       V
```

### Documentation (`/docs`)

The `/docs` module serves as the authoritative, user-facing documentation hub for the Pinpoint bisection tool.

*   **Design Philosophy**: Prioritizes immediate utility and actionable guidance. It adopts a problem-oriented approach, directly addressing common user challenges (e.g., troubleshooting aborted jobs, explaining error conditions) rather than merely listing features. This aims to minimize user frustration and accelerate issue resolution.
*   **Key Components:**
    *   **`abort_error.md`**: Guides users through troubleshooting and rerunning aborted jobs (e.g., due to exceeding a build limit). It provides strategic advice on interpreting partial results and refining subsequent bisection efforts, transforming a seemingly failed job into an opportunity for focused investigation.
    *   **`errors.md`**: A comprehensive reference for Pinpoint's error messages. It demystifies cryptic error codes by providing clear explanations of underlying causes and precise "Suggested course of action" for each, empowering users to self-diagnose and resolve issues without immediate team intervention. Errors are categorized by domain (test output, Swarming, build failures, internal errors).
    *   **`/docs/doc_imgs` directory**: Hosts visual assets (e.g., `large_CL_range.png`, `many_changes.png`, `job_failure.png`) that enhance textual documentation by illustrating complex scenarios within the Pinpoint UI or conceptual model, providing tangible representations.

**User Workflow for Troubleshooting**

```
User initiates Pinpoint job
      |
      V
  Job execution progresses...
      |
      +----[ Job aborts (e.g., build cap exceeded) ]----+
      |                                                  |
      +----[ Specific error message displayed        ]----+
      |                                                  |
      V                                                  V
Consult `abort_error.md` (for aborted jobs)         Consult `errors.md` (for specific errors)
      |                                                  |
      V                                                  V
Identify scenario (e.g., "CL range too long")      Diagnose problem (e.g., "Test failed, no output")
      |                                                  |
      +------------------> Follow Suggested Course of Action <-------------------+
                                      |
                                      V
             (e.g., Rerun job with smaller CL range, apply a patch to fix a build,
                    file a bug with the relevant team, or investigate logs further)
                                      |
                                      V
                      Achieve successful bisection or appropriate escalation
```

### Backend Testing (`/test.py`)

*   **`test.py`**:
    *   **Responsibility**: Defines the base `TestCase` class for Pinpoint's backend unit and integration tests. It sets up the `webtest.TestApp` for HTTP request simulation, configures mocking for external services (`gitiles_service`, `gerrit_service`), and populates initial repository data.
    *   **Design Rationale**: Provides a consistent and isolated environment for testing backend logic. By using `unittest.mock` for external services like Gitiles and Gerrit, tests become fast, deterministic, and self-contained, avoiding reliance on actual network calls or external system states. `webtest` facilitates testing HTTP handlers directly. The provided stub functions (`_CommitInfoStub`, `_CommitRangeStub`) simulate complex scenarios, such as non-linear Git history, crucial for robust bisection testing without real Git repositories.







# Module: /docs

The `/docs` module serves as the authoritative, user-facing documentation hub for the Pinpoint bisection tool. Its central purpose is to empower users to effectively understand, diagnose, and resolve common issues encountered during Pinpoint job execution, thereby streamlining the process of identifying performance regressions and other code-related changes.

The design of this module prioritizes immediate utility and actionable guidance. Instead of merely describing Pinpoint's features, it adopts a problem-oriented approach, directly addressing the challenges users face. This is evident in its focus on troubleshooting aborted jobs, explaining various error conditions, and providing concrete steps for resolution. This structure is a deliberate choice to minimize user frustration and accelerate the path to a successful bisection or appropriate escalation.

### Responsibilities and Key Components

*   **`abort_error.md`**
    This document guides users through troubleshooting and rerunning Pinpoint jobs that have prematurely terminated, typically due to exceeding a maximum build limit. It doesn't just list reasons for job abortion; it provides strategic advice on how to interpret partial results and refine subsequent bisection efforts. The "why" behind its content is to transform a seemingly failed job into an opportunity for focused investigation, teaching users to intelligently narrow down problem spaces.
    *   **How it works:** It presents common scenarios that lead to aborted jobs—such as excessively long Change List (CL) ranges, the detection of numerous changes (making the signal noisy), or underlying build/test failures. For each scenario, it offers specific, actionable recommendations, such as identifying the largest regression to focus the next run, prioritizing more recent or significant changes, or investigating test logs to apply corrective patches. The document aims to prevent blind reruns and encourages an informed, iterative approach to bisection.

*   **`errors.md`**
    This document serves as a comprehensive reference for various error messages Pinpoint can generate during job execution. Its core responsibility is to demystify cryptic error codes by providing clear explanations of their underlying causes and precise "Suggested course of action" for each. The "why" here is to empower users to self-diagnose and potentially resolve issues without immediate intervention from the Pinpoint team, thereby reducing support load and increasing user autonomy.
    *   **How it works:** Errors are meticulously categorized by their domain, ranging from issues with test output and metric parsing (`#ReadValueNoFile`, `#ReadValueNotFound`) to problems with the Swarming infrastructure (`#SwarmingExpired`, `#SwarmingNoBots`), build failures (`#BuildFailed-INFRA-FAILURE`, `#BuildFailed-BUILD-FAILURE`), and internal Pinpoint errors (`#FATAL-ERROR-MSG`). For each entry, it provides a detailed explanation of what went wrong (e.g., test didn't produce expected output, Swarming bots were unavailable, a build recipe broke) and then directs the user on how to proceed, often by filing a bug with a specific team, attempting a rerun, or applying a patch to resolve a broken build.

*   **`/docs/doc_imgs` directory**
    This dedicated directory is responsible for hosting and managing visual assets that enhance the clarity and understanding of the textual documentation. The decision to separate images into this module stems from a need to maintain a clean content structure, prevent documentation files from being cluttered with binary data, and ensure consistent referencing. The "why" for including images is to make complex scenarios more immediately understandable, providing concrete visual anchors that text alone might struggle to convey.
    *   **How it works:** Each image within this directory is specifically chosen to illustrate a particular problem or state within the Pinpoint user interface or conceptual model:
        *   `large_CL_range.png`: Visually highlights the detrimental effect of an overly broad commit range on a bisection job, reinforcing the documentation's advice to narrow the scope.
        *   `many_changes.png`: Illustrates the challenge of interpreting multiple, potentially noisy regression points, grounding the recommendation to focus on more significant or recent changes.
        *   `job_failure.png`: Provides immediate visual recognition of a failed job state, aiding users in quickly identifying and understanding discussions around troubleshooting build or test failures. These images serve as direct examples, transforming abstract concepts into tangible representations.

### User Workflow for Troubleshooting

When a user encounters an issue with a Pinpoint job, the documentation provides a clear pathway to resolution:

```
User initiates Pinpoint job
      |
      V
  Job execution progresses...
      |
      +----[ Job aborts (e.g., build cap exceeded) ]----+
      |                                                  |
      +----[ Specific error message displayed        ]----+
      |                                                  |
      V                                                  V
Consult `abort_error.md` (for aborted jobs)         Consult `errors.md` (for specific errors)
      |                                                  |
      V                                                  V
Identify scenario (e.g., "CL range too long")      Diagnose problem (e.g., "Test failed, no output")
      |                                                  |
      +------------------> Follow Suggested Course of Action <-------------------+
                                      |
                                      V
             (e.g., Rerun job with smaller CL range, apply a patch to fix a build,
                    file a bug with the relevant team, or investigate logs further)
                                      |
                                      V
                      Achieve successful bisection or appropriate escalation
```




# Module: /docs/doc_imgs

The `/docs/doc_imgs` module provides a dedicated repository for visual assets used across the project's documentation. Its fundamental purpose is to augment textual explanations with concrete visual examples, thereby enhancing comprehension, clarifying complex scenarios, and providing immediate context for concepts that are challenging to convey through text alone. This centralization of documentation-specific imagery ensures consistency, facilitates easier management, and promotes a more accessible and engaging documentation experience.

The module's design decision to consolidate images in a separate directory stems from the need to maintain a clear separation between content and supporting media. This approach prevents documentation directories from becoming cluttered with binary files, simplifies content management, and allows for consistent referencing patterns across various documentation files. The specific images included are chosen to illustrate common operational states, challenges, or system behaviors that are often difficult to fully grasp without a visual aid. These images serve as direct examples that anchor abstract concepts to tangible representations.

Key components and their responsibilities within this module are the individual image files:

*   **`job_failure.png`**: This image is responsible for visually depicting a system job in a failed state. Its primary role is to provide a clear, unambiguous illustration of what a job failure looks like within the system's user interface or logs. This helps readers quickly recognize failure patterns, understand associated error messages, and grasp the context of documentation discussing error handling, debugging, or recovery procedures. It acts as a visual anchor for troubleshooting guides and explanations of system resilience.

*   **`large_CL_range.png`**: This image is designed to illustrate a Code Change List (CL) or a set of modifications that spans an unusually broad range of revisions or files. It serves to visually explain the implications of extensive code changes, which can impact review complexity, integration challenges, or performance characteristics of tools processing these changes. The image provides a concrete example for documentation discussing best practices for atomic changes, code review guidelines for large submissions, or performance considerations when dealing with significant diffs.

*   **`many_changes.png`**: This image provides a visual representation of a scenario characterized by a high volume of individual modifications, often spread across numerous files, within a specific change set (e.g., a single commit or CL). Its responsibility is to differentiate between a "large range" (which could be a few very large files) and "many changes" (which implies numerous, potentially smaller, modifications across a wide array of files). This visual distinction is crucial for documentation explaining metrics related to change complexity, strategies for managing widespread impacts, or discussions around modularity and decomposition of work.







# Module: /elements

The `/elements` module forms the core user interface layer of the application, encompassing a comprehensive collection of reusable Web Components built using Polymer. Its primary responsibility is to translate raw data from the performance analysis system into interactive, understandable, and actionable information, enabling users to monitor, configure, and debug performance analysis jobs efficiently.

The design of the `/elements` module is driven by a commitment to creating a modular, maintainable, and user-centric front-end application.

*   **Component-Based Architecture (Polymer):** The selection of Polymer for building Web Components is foundational. This architecture fosters strong encapsulation of UI logic, styling, and behavior within individual elements, significantly simplifying development, testing, and reuse. This modularity is particularly evident in how complex features, such as job details or job creation, are broken down into smaller, manageable sub-components, each with a clear, isolated responsibility.

*   **Consistent Styling and Theming (`base-style.html`):** A dedicated `base-style.html` component ensures a unified visual language across the entire application. By defining CSS variables and common styles for typography, tables, links, and specialized commit renderings, the module guarantees a cohesive aesthetic. The introduction of z-index variables (e.g., `--layer-menus`, `--layer-dialogs`) is a strategic choice to manage overlapping UI elements predictably, preventing visual glitches and ensuring critical components like dropdown menus appear correctly layered.

*   **Standardized Data Loading and Error Handling (`loading-wrapper.html`):** Recognizing the prevalence of asynchronous data fetching, the `loading-wrapper` component provides a standardized pattern for indicating loading states and displaying errors. This abstractive approach centralizes common UI patterns, reducing boilerplate code in individual pages and ensuring a consistent user experience during data operations. Users always receive clear feedback on whether data is being fetched or if an error has occurred, which improves perceived responsiveness and reduces frustration.

*   **Enhanced User Input (`autocomplete-box.html`):** For fields requiring specific, often numerous, input values (e.g., bot names, benchmarks, commit hashes), the `autocomplete-box` component significantly improves usability. By offering real-time suggestions, grouping, and tag support, it guides users towards valid selections, minimizes typing errors, and accelerates form completion. This component is a critical abstraction for handling complex selection processes effectively.

*   **Client-Side Routing and Application Shell (`index-page.html`):** The `index-page` serves as the application's main shell, managing client-side routing using `app-location` and `app-route`. This design choice creates a Single Page Application (SPA) experience, where navigation between different views (e.g., jobs list, job details, statistics) occurs without full page reloads, resulting in a faster, more fluid user experience. `iron-pages` then dynamically swaps content based on the active route, ensuring only the relevant components are rendered.

*   **Contextual Feedback and Actions (`paper-toast`, `paper-dialog`):** Throughout the module, `paper-toast` elements provide ephemeral, non-intrusive user feedback (e.g., "command copied!"). More complex interactions, such as confirming job cancellations (`cancel-job-dialog`) or providing detailed help (`job-search-tip`, `change-details` reproduction help), leverage `paper-dialog` for modal, focused interaction, preventing users from losing context. This hierarchy of feedback mechanisms enhances user understanding and control.

*   **Integration with External Services (`navigation-bar.html`, `job-page.html`):** The module is designed to integrate seamlessly with external services. The `navigation-bar` includes Google Sign-In for authentication and automatically populates bug reports with current application state, streamlining user support. Components within `job-page` provide direct links to source code, bug trackers, and detailed analysis tools, positioning the application as a central hub for performance investigation without reimplementing external functionalities.

### Responsibilities and Key Components

The `/elements` module is comprised of several distinct components and sub-modules, each contributing to the overall application functionality:

*   **Core UI Elements:**
    *   **`base-style.html`**: Establishes global CSS variables (e.g., `--layer-menus` for z-index management), typography, and common styling patterns used consistently across all other components, ensuring a unified visual identity.
    *   **`loading-wrapper.html`**: A ubiquitous utility component that wraps content, displaying a `paper-spinner` while data is being fetched asynchronously and presenting error messages if requests fail. This centralizes visual feedback for data operations, improving user experience by clearly communicating application state.
    *   **`autocomplete-box.html`**: Provides a robust input field with auto-completion capabilities. It assists users by suggesting items, supporting grouping and optional tags, and handling keyboard navigation. This enhances data entry accuracy and speed for fields with predefined or numerous options.

*   **Application Structure and Navigation:**
    *   **`index-page.html`**: The main application container that orchestrates client-side routing, dynamically loads different content pages (e.g., jobs list, job details, statistics) using `iron-pages`, and manages global application state, including user authentication and client-side API authorization headers.
    *   **`navigation-bar.html`**: The persistent top navigation bar, offering quick access to the application's home, documentation, and a bug reporting mechanism. It integrates Google Identity Services for user sign-in and sign-out, displaying the user's status and managing session-related information. It also intelligently populates bug reports with current page context.
    *   **`error-page.html`**: A fallback component displayed when a requested route does not match any known page, providing a clear "Page not found" message to the user.

*   **Job Management and Monitoring:**
    *   **`jobs-page` (sub-module, rooted at `jobs-page.html`)**: This comprehensive sub-module is dedicated to *creating and listing* performance analysis jobs.
        *   **Responsibility:** Provides the user interface for initiating, configuring, and monitoring performance analysis jobs (such as try jobs and bisects). It acts as the primary interaction point for users to define the parameters of a job, view its status, and access detailed results. The module's design emphasizes dynamic forms, real-time data fetching, and user-friendly commit selection to streamline the job creation process.
        *   **Key Functionality:**
            *   **`attempt-count-help.html`**: Provides users with detailed, on-demand information regarding the default attempt counts for performance jobs, particularly for specific hardware configurations and benchmarks where default iteration counts are automatically adjusted.
            *   **`commit-input.html`**: Facilitates accurate and efficient input of Git commit hashes or other commit-related identifiers by providing real-time autocompletion suggestions.
            *   **`commit-details.html`**: Retrieves and displays comprehensive details for a given Git commit hash, including the full hash, commit message, author, and links to the review or source code, providing crucial context.
            *   **`jobs-table.html`**: Presents a structured, sortable list of performance jobs, offering an overview of each job, its status, and creation date, along with options for interacting with specific jobs, including cancellation.
            *   **`new-job-fab.html`**: The core component for creating and configuring new performance jobs (both "Try Job" for A/B comparisons and "Bisect" for regression pinpointing). It integrates other components to form a comprehensive job submission interface, pre-populating forms for reruns and handling submission to the backend.
    *   **`job-page` (sub-module, rooted at `job-page.html`)**: This equally significant sub-module is designed for *in-depth viewing and analysis of a single job*.
        *   **Responsibility**: Presents detailed information about a single "job" within the performance analysis system. Its primary goal is to enable users to understand the context, results, and potential issues of a job, and to facilitate troubleshooting and further investigation, including local reproduction of test environments and benchmark runs. It acts as a comprehensive dashboard for a specific job, aggregating various data points into a cohesive, interactive view.
        *   **Key Functionality**:
            *   **`change-details.html`**: A central component for displaying the specifics of a selected code change, including commit details (`change-info`), a grid of test attempts and execution statuses, and detailed execution breakdowns. Crucially, it provides local reproduction utilities (commands for downloading binaries and running benchmarks) and accompanying help dialogs.
            *   **`change-info.html`**: A smaller, reusable component focused on presenting summary information about a single code change, including subject, author, Git hash, and links to source or review.
            *   **`exception-details.html`**: Dedicated to rendering detailed error information, including the exception message and a toggleable full traceback, to aid in troubleshooting failed executions.
            *   **`execution-details.html`**: Presents fine-grained, technical details and outputs of a single test execution or "quest", including key-value metadata, links to logs, and a "swarming reproduce" command for replaying tasks locally. It integrates `exception-details` for failure display.
            *   **`job-chart.html`**: The primary visualization component using D3.js to track job results across a sequence of code changes, highlighting trends, regressions, and improvements. It provides an interactive chart for selecting specific changes.
            *   **`job-details.html`**: Displays general information and configuration about the entire job, including external links to bug reports, analysis tools, raw results, and a command-line reproduction snippet for the entire job.
            *   **`job-menu-fab.html`**: A floating action button component for initiating job-related actions, such as creating new jobs based on existing parameters.
    *   **`cancel-job-dialog.html`**: A modal dialog, integrated into job listing and detail views, that enables users to cancel queued or running jobs. It requires a reason for cancellation, promoting accountability.
    *   **`results2-frame.html`**: An iframe wrapper component for embedding external "Results2" pages. It handles the asynchronous loading of these complex performance reports, displays status, and provides warnings for compatibility issues with older reports.

*   **System Performance and Operational Statistics:**
    *   **`queue-stats-page.html`**: Provides real-time statistics for a specific job queue, including the number of queued and running jobs, a D3.js-powered histogram visualizing queue waiting times, and a list of jobs currently in the queue. This offers transparency into resource utilization and potential delays.
    *   **`stats-page.html`**: Displays aggregate operational metrics for the entire performance analysis system. It presents tables summarizing job reliability (reproducibility, failure rates) and latency (job durations at various percentiles) over different timeframes (7/28 days), including D3.js histograms for job duration distributions.
    *   **`migrate-page.html`**: A specialized administrative page that provides a user interface for initiating and monitoring backend data migration processes. It displays real-time progress, including completed counts and errors, allowing for oversight of critical system maintenance tasks.

### Key Workflows

**1. General Application Navigation and Authentication**

```
User opens application
       |
       v
[index-page] loads, initializing routing and authentication client.
       |
       v
[navigation-bar] checks user's login status (via cookie/Google Identity).
       |
       v
[navigation-bar] displays "Sign In" button if not logged in.
       |
       | (User clicks "Sign In")
       v
Google Identity pop-up appears; user authenticates.
       |
       v
[navigation-bar] updates `user` property, displays "Sign Out" and user's email.
       |
       v
[index-page] receives `user` update and can pass it down to other pages.
       |
       v
[index-page] routes to default page (e.g., "jobs") or requested page.
       |
       v
Respective page (e.g., [jobs-page]) fetches data, potentially using `user` for filtering.
```

**2. Initiating and Configuring a New Job (detailed in `/elements/jobs-page`)**

```
User Clicks "Add" (or "Refresh" to restart job)
      |
      v
`new-job-fab` opens the job creation dialog
      |
      v
`new-job-fab` fetches available Bots and Benchmarks from backend APIs
      |
      v
User selects Bot ("configuration") and Benchmark
      |
      v
`new-job-fab` fetches detailed Benchmark Configuration (stories, metrics)
      |
      v
User selects Job Type: "Try Job" (A/B) or "Bisect" (regression)
      |
      v
If "Try Job": User specifies Base and Experiment Git Hashes/Builds and optional patches/args
      |
      v
If "Bisect": User specifies Start and End Git Hashes for the bisect range, and the metric to track
      |
      v
(Throughout commit selection, `commit-details` fetches and displays commit info)
      |
      v
User fills in other details (Story, Story Tags, Project, Bug ID, Batch ID, Extra Test Args)
      |
      v
User Clicks "Start"
      |
      v
`new-job-fab` validates inputs and submits form data to the backend `/api/new`
      |
      v
On successful job creation: `app-location` redirects user to `/job/<jobId>`
      |
      v
On submission error: `new-job-fab` displays error message within the dialog
```

**3. Exploring Job Performance Over Time (detailed in `/elements/job-page`)**

```
User navigates to a job page
       |
       v
[job-details] - Shows overall job arguments, links to bug/analysis.
       |
       v
[job-chart]   - Visualizes performance metric across multiple commits.
                Highlights detected differences.
                Allows user to select a specific commit for detail.
       |
       | (User selects a commit)
       v
[change-details] - Displays details for the selected commit:
                     - Commit info (`change-info`).
                     - Grid of attempts/executions status.
                     - Prompts user to select an attempt/execution for further details.
       |
       | (User selects an attempt/execution)
       v
[execution-details] - Shows specific metrics and metadata for that execution.
                     - If failed, integrates [exception-details].
```

**4. Reproducing a Failing Execution Locally (detailed in `/elements/job-page`)**

```
User identifies a failed execution in [change-details] (e.g., a red box in the status grid).
       |
       v
User selects the failed execution.
       |
       v
[execution-details] - Displays detailed metadata, including the "task" ID.
                     - Provides a "🖥️" button to copy "swarming reproduce" command.
                     - Provides a "❓" button to open a help dialog for swarming setup.
       |
       | (User copies Swarming command)
       v
(User executes command in local terminal)
```

**5. Monitoring System Queue Health**

```
User navigates to the Queue Stats page (e.g., /queue-stats/linux-perf)
       |
       v
[queue-stats-page] fetches queue data for the specified configuration.
       |
       v
[queue-stats-page] displays:
   - Number of queued and running jobs.
   - A D3.js histogram of recent queue waiting times.
   - A list of `job_id` and `status` for jobs currently in that queue.
       |
       | (User clicks a job ID in the list)
       v
Application navigates to [job-page] for the selected job.
```









# Module: /elements/job-page

The `/elements/job-page` module is responsible for presenting detailed information about a single "job" within the performance analysis system. Its primary goal is to enable users to understand the context, results, and potential issues of a job, and to facilitate troubleshooting and further investigation, including local reproduction of test environments and benchmark runs. It acts as a comprehensive dashboard for a specific job, aggregating various data points into a cohesive, interactive view.

### Design Decisions and Implementation Choices

The module is structured as a collection of Polymer web components, promoting modularity, reusability, and a consistent user experience. This component-based approach allows for clear separation of concerns, with each sub-element handling a specific aspect of the job's display or functionality.

*   **Data Visualization (using D3.js)**: The `job-chart` component leverages D3.js for dynamic and interactive data visualization. This choice is driven by the need to represent complex performance data (e.g., statistical distributions of results across changes) in a clear and interpretable graphical format. D3's flexibility allows for custom visualizations like histograms with mean lines, and features like percentile-based Y-axis scaling (`d3.quantile`) are employed to handle potential data outliers and provide a more representative view of trends.
*   **Progressive Disclosure of Information**: The module adopts a progressive disclosure pattern to manage information complexity. A high-level chart (`job-chart`) provides an overview of results across changes; clicking on a change reveals `change-details`, showing attempts and executions in a compact grid; selecting an attempt then exposes `execution-details` with fine-grained metadata; and finally, exception details (`exception-details`) are collapsed by default, requiring a user click to expand the full traceback. This prevents overwhelming the user with excessive detail unless specifically requested for troubleshooting.
*   **Actionable Insights and Local Reproduction**: A key design goal is to empower users to act on the information presented. The "Reproduce Locally" feature in `change-details` and "Swarming reproduce" in `execution-details` directly provide commands (CAS download, benchmark run, Swarming task replay). This streamlines the debugging process by minimizing manual command construction and potential errors. Accompanying help dialogs (`paper-dialog`) explain the prerequisites and usage of these commands, anticipating common user questions and reducing the barrier to local reproduction.
*   **Contextual Information with External Links**: The `job-details` component aggregates links to relevant external systems (bug trackers, CABE analysis, source code search for benchmarks). This design choice ensures that the job page serves as a central hub, enabling quick navigation to related resources for deeper analysis or context without duplicating information.
*   **User Feedback**: `paper-toast` elements are used to provide immediate, non-intrusive feedback to the user when actions like copying commands are completed, enhancing the perceived responsiveness of the application.

### Responsibilities and Key Components

*   **`change-details.html`**: This is a central component for displaying the specifics of a selected code change within a job.
    *   **Role**: It provides a granular view of how a particular code change performed across multiple test *attempts* and *executions* (quests). It's designed to help users identify and understand the impact of a specific commit on job results.
    *   **Key Functionality**:
        *   **Commit Summary**: Integrates `change-info` to show commit details (subject, author, links).
        *   **Attempt & Execution Status Grid**: Visualizes all attempts for a given change and the status (completed, failed, pending, running) of each individual execution (quest) within those attempts using a color-coded grid. This allows for quick identification of unstable or failing runs.
        *   **Detailed Execution Breakdown**: When a specific attempt and quest are selected, it dynamically displays their full details using `execution-details`.
        *   **Local Reproduction Utilities**: Offers buttons to copy commands for downloading Chrome binaries (from CAS) and running benchmarks locally. These features are critical for developers to debug performance regressions or verify fixes in their local environment.
        *   **User Help**: Includes a `paper-dialog` to explain the local reproduction commands and their prerequisites.

*   **`change-info.html`**: A smaller, reusable component focused on presenting summary information about a single code change.
    *   **Role**: To provide a concise, readable summary of a commit's metadata, making it easy to identify the change and navigate to its source or review.
    *   **Key Functionality**: Displays the commit subject, author, Git hash, and provides clickable links to the code review or commit page. It also optionally shows extra arguments relevant to the change.

*   **`exception-details.html`**: This component is dedicated to rendering detailed error information.
    *   **Role**: When an execution fails, this component makes the exception message and traceback accessible in an organized manner, aiding in troubleshooting.
    *   **Key Functionality**: Displays the exception message prominently. It also implements a "Show More/Hide" toggle to reveal the full traceback, preventing long stack traces from cluttering the UI by default. It includes a link to documentation on Pinpoint errors for further assistance.

*   **`execution-details.html`**: Presents the specific details of a single test execution or "quest".
    *   **Role**: To expose the fine-grained, technical details and outputs of an individual test run, crucial for deep-dive analysis and debugging.
    *   **Key Functionality**: Displays key-value pairs of execution metadata (`details`), often including links to logs, results, or task IDs. Significantly, it provides a "swarming reproduce" command, allowing developers to replay the exact test setup on their local machine. It also integrates `exception-details` to display any failures.

*   **`job-chart.html`**: The primary visualization component for tracking job results across a sequence of code changes.
    *   **Role**: To provide a visual historical context of a job's performance over time (or across a bisect range), highlighting trends, regressions, and improvements. It enables quick identification of impactful changes.
    *   **Key Functionality**:
        *   **Interactive Chart**: Uses D3.js to render a chart where each vertical column represents a code change.
        *   **Result Visualization**: For each change, it displays a histogram of result values (if available) or an icon (if results are pending or unavailable).
        *   **Difference Highlighting**: Visually emphasizes "different" changes (e.g., statistically significant performance shifts) and indicates the magnitude of the difference from the previous change.
        *   **Change Selection**: Allows users to select a specific change, triggering updates in `change-details`.
        *   **Improvement Direction**: Clearly indicates whether "higher is better" or "lower is better" for the displayed metric.

*   **`job-details.html`**: Displays general information and configuration about the entire job.
    *   **Role**: To provide the overarching context for the job, including its purpose, configuration, and links to related external resources.
    *   **Key Functionality**:
        *   **External Links**: Provides direct links to bug reports (Crbug, internal issues), comprehensive analysis tools (CABE), and raw benchmark results.
        *   **Job Arguments**: Lists all arguments used to configure the job, with smart linking to source code search for benchmark/story definitions.
        *   **Command-Line Reproduction**: Generates a complete `pinpoint experiment-telemetry-start` command, enabling users to re-run the entire job setup from their local terminal, particularly useful for try-jobs.

*   **`job-menu-fab.html`**: A floating action button component for initiating job-related actions.
    *   **Role**: To provide an easily accessible and visually consistent entry point for high-level actions pertinent to the current job, such as creating new jobs based on existing parameters.
    *   **Key Functionality**: Toggles a submenu which, in its current state, contains a "New Job" button (provided by `new-job-fab`). This allows users to quickly initiate follow-up actions like re-running a job or starting a bisect based on the current job's context.

### Key Workflows

**1. Exploring Job Performance Over Time**

```
User navigates to a job page
       |
       v
[job-details] - Shows overall job arguments, links to bug/analysis.
       |
       v
[job-chart]   - Visualizes performance metric across multiple commits.
                Highlights detected differences.
                Allows user to select a specific commit for detail.
       |
       | (User selects a commit)
       v
[change-details] - Displays details for the selected commit:
                     - Commit info (`change-info`).
                     - Grid of attempts/executions status.
                     - Prompts user to select an attempt/execution for further details.
       |
       | (User selects an attempt/execution)
       v
[execution-details] - Shows specific metrics and metadata for that execution.
                     - If failed, integrates [exception-details].
```

**2. Reproducing a Failing Execution Locally**

```
User identifies a failed execution in [change-details] (e.g., a red box in the status grid).
       |
       v
User selects the failed execution.
       |
       v
[execution-details] - Displays detailed metadata, including the "task" ID.
                     - Provides a "🖥️" button to copy "swarming reproduce" command.
                     - Provides a "❓" button to open a help dialog for swarming setup.
       |
       | (User copies Swarming command)
       v
(User executes command in local terminal)
```

**3. Setting up a Local Environment to Run Benchmarks**

```
User wants to reproduce the test environment for a specific commit.
       |
       v
User is on [change-details] for the relevant commit.
       |
       v
[change-details] - Provides a "⬇️" button to copy the CAS download command
                   (to get Chrome binaries).
                 - Provides a "🖥️" button to copy the benchmark run command.
                 - Provides "Reproduce Locally" link to open a help dialog for
                   CAS/depot_tools setup.
       |
       | (User copies CAS download command)
       v
(User executes CAS command to download Chrome binaries)
       |
       | (User copies benchmark command)
       v
(User executes benchmark command with downloaded binaries)
```













# Module: /elements/jobs-page

This module, `/elements/jobs-page`, provides the user interface for initiating, configuring, and monitoring performance analysis jobs (such as try jobs and bisects). It acts as the primary interaction point for users to define the parameters of a job, view its status, and access detailed results. The module's design emphasizes dynamic forms, real-time data fetching, and user-friendly commit selection to streamline the job creation process.

## Design Decisions and Implementation Choices

The module is built using Web Components (specifically Polymer), which underpins its modularity and component-based structure. This approach allows for the encapsulation of UI logic, styling, and behavior within individual elements, promoting reusability and maintainability across the application.

Key design considerations include:

*   **Dynamic Form Generation:** Job creation forms are highly dynamic, adapting to user selections (e.g., switching between "Try Job" and "Bisect" modes) and dynamically fetching relevant data from backend APIs (e.g., available bots, benchmarks, commit details). This ensures users only see relevant options, reducing cognitive load.
*   **Asynchronous Data Loading:** `iron-ajax` is used extensively for fetching configuration data, commit details, and benchmark specifics. This asynchronous design prevents UI freezes and allows for a responsive user experience, even when dealing with potentially slow network requests or large datasets.
*   **Enhanced User Input:** `autocomplete-box` (a common behavior) is integrated into several input fields, guiding users by suggesting valid options for complex fields like bot configurations, benchmarks, and commit hashes. This reduces input errors and accelerates form completion.
*   **Contextual Information and Validation:** Components like `attempt-count-help` provide on-demand explanations for specific form fields, improving user understanding without cluttering the main interface. Client-side validation using `iron-form` and `IronValidatableBehavior` is employed to catch common input errors before submission.
*   **Job Lifecycle Management:** The module supports not only job creation but also viewing job status and enabling actions like cancellation for running jobs, providing users with comprehensive control over their initiated tasks.
*   **Pre-population for Reruns:** When restarting an existing job, the system intelligently pre-populates the new job form with parameters from the original job. This significantly simplifies the process of re-running or modifying jobs based on previous configurations.

## Responsibilities and Key Components

The `/elements/jobs-page` module is composed of several specialized Web Components, each handling a distinct part of the job management workflow:

*   **`attempt-count-help.html`**
    *   **Responsibility:** To provide users with detailed, on-demand information regarding the default attempt counts for performance jobs. This is particularly relevant for specific hardware configurations (like Mac M1) and benchmarks where default iteration counts are automatically adjusted to improve regression detection accuracy.
    *   **How it works:** It presents a small "help" icon which, when clicked, opens a `paper-dialog` containing a formatted explanation of the attempt count defaults and their underlying rationale, including links to further documentation.

*   **`commit-input.html`**
    *   **Responsibility:** To facilitate the accurate and efficient input of Git commit hashes or other commit-related identifiers (like commit positions or "recent build" references). It's designed to assist users in selecting the correct commit by providing real-time suggestions.
    *   **How it works:** It extends a standard `paper-input` with autocompletion capabilities. As the user types, it filters a predefined list of suggested commits (e.g., recent builds for a specific configuration) and displays them in a dropdown menu. It also handles keyboard navigation for selection and allows displaying brief commit details for each suggestion.

*   **`commit-details.html`**
    *   **Responsibility:** To retrieve and display comprehensive details for a given Git commit hash. This includes information such as the full hash, commit message, author, commit position, and links to the review or source code, providing crucial context for the selected commit.
    *   **How it works:** It integrates with `commit-input` to receive a commit hash. Upon receiving a valid hash, it makes an asynchronous call to the backend `/api/commit` endpoint using `iron-ajax`. During loading, it shows a progress indicator, and upon success, it renders the parsed commit details. It also handles and displays any errors encountered during the API request.

*   **`jobs-table.html`**
    *   **Responsibility:** To present a structured, sortable list of performance jobs. This table offers an overview of each job, including its name, configuration, status, and creation date, along with options for interacting with specific jobs.
    *   **How it works:** It renders a dynamic HTML table where each row represents a job. Columns are sortable by clicking their headers, providing an intuitive way to organize job lists. It dynamically formats various data fields (e.g., dates, email addresses, comparison modes) for readability. Critically, it integrates a `cancel-job-dialog` for "Queued" or "Running" jobs, allowing users to stop ongoing tasks.

*   **`new-job-fab.html`**
    *   **Responsibility:** This is the core component for creating and configuring new performance jobs (both "Try Job" for A/B comparisons and "Bisect" for regression pinpointing). It brings together all other components to form a comprehensive job submission interface.
    *   **How it works:** It presents a `paper-dialog` containing a multi-section form (`iron-form`). It uses `iron-ajax` to fetch lists of available bot configurations, test suites (benchmarks), and detailed benchmark configurations (stories, metrics). User selections dynamically influence available options and form structure. It utilizes `commit-details` for robust commit selection and applies logic for `defaultAttemptCount` based on bot and benchmark. After successful submission to the `/api/new` endpoint, `app-location` is used to navigate the user to the newly created job's details page. It also handles pre-populating the form when a user chooses to restart an existing job, copying its parameters for convenience.

## Key Workflows

### 1. Initiating and Configuring a New Job

```text
User Clicks "Add" (or "Refresh" to restart job)
      |
      v
`new-job-fab` opens the job creation dialog
      |
      v
`new-job-fab` fetches available Bots and Benchmarks from backend APIs
      |
      v
User selects Bot ("configuration") and Benchmark
      |
      v
`new-job-fab` fetches detailed Benchmark Configuration (stories, metrics)
      |
      v
User selects Job Type: "Try Job" (A/B) or "Bisect" (regression)
      |
      v
If "Try Job": User specifies Base and Experiment Git Hashes/Builds and optional patches/args
      |
      v
If "Bisect": User specifies Start and End Git Hashes for the bisect range, and the metric to track
      |
      v
(Throughout commit selection, `commit-details` fetches and displays commit info)
      |
      v
User fills in other details (Story, Story Tags, Project, Bug ID, Batch ID, Extra Test Args)
      |
      v
User Clicks "Start"
      |
      v
`new-job-fab` validates inputs and submits form data to the backend `/api/new`
      |
      v
On successful job creation: `app-location` redirects user to `/job/<jobId>`
      |
      v
On submission error: `new-job-fab` displays error message within the dialog
```

### 2. Viewing and Managing Existing Jobs

```text
User navigates to the jobs listing page
      |
      v
`jobs-table` receives a list of job objects
      |
      v
`jobs-table` renders each job as a row, displaying key details (Name, Configuration, Status, etc.)
      |
      v
User interacts with the table:
  - Clicks a column header: `jobs-table` re-sorts the list
  - Clicks a Job Name: Navigates to the individual job details page
  - For "Queued" or "Running" jobs, a "Cancel" option is shown:
        User Clicks "Cancel"
              |
              v
        `cancel-job-dialog` opens for confirmation
              |
              v
        User Confirms Cancellation
              |
              v
        `cancel-job-dialog` sends request to backend to cancel job
              |
              v
        Job status in `jobs-table` updates to "Cancelled"
```



















# Module: /handlers

The `/handlers` module serves as the primary interface layer for the Pinpoint service, exposing both external API endpoints for clients and internal cron job handlers for automated operations. It is responsible for receiving and processing all incoming requests, orchestrating job creation, managing job lifecycle events, and providing job status and configuration information. This module bridges the client-facing and internal logic layers, ensuring that user requests and automated system events are properly interpreted, validated, and dispatched to the core Pinpoint models and services.

### Key Components and Responsibilities

The module is composed of several specialized handlers, each designed for a distinct set of responsibilities:

*   **`new.py` - Job Creation:**
    This handler is the entry point for initiating new Pinpoint jobs. Its core responsibility is to receive user requests, perform extensive validation of the provided parameters (such as `comparison_mode`, `target`, `bug_id`, `changes`, `priority`, and `tags`), and then translate these into an executable Pinpoint job. A key design decision here is the use of `_ArgumentsWithConfiguration`, which merges user-provided arguments with predefined bot configurations, ensuring consistency and ease of use while allowing for flexible overrides. The `_GenerateQuests` function dynamically determines the sequence of operations (e.g., `FindIsolate`, `RunTelemetryTest`, `ReadValue`) required for the specific test type requested, abstracting the underlying execution details. Once validated and constructed, the job is immediately scheduled using the `scheduler` module, which accounts for job cost during queueing, and then initial post-creation updates (like posting a bug comment) are triggered.

*   **`job.py` and `jobs.py` - Job Retrieval and Management:**
    *   **`job.py`:** This handler provides an API endpoint for retrieving detailed information about a single Pinpoint job. It supports various output options to tailor the response. Uniquely, `job.py` also provides an endpoint (`JobHandlerPost`) for external systems, specifically Skia, to *push* a complete job state into Pinpoint. This design choice enables specialized integrations where an external service manages its own job lifecycle and then propagates the full, pre-computed state to Pinpoint for historical tracking, reporting, or specific post-processing. The `MarshalToJob`, `MarshalToState`, `MarshalToChange`, and `MarshalToAttempt` functions facilitate this by converting the external data format into Pinpoint's internal model structures, effectively bypassing the normal scheduling and execution flow for these pushed jobs.
    *   **`jobs.py`:** This handler is dedicated to providing an overview of multiple Pinpoint jobs. It supports robust filtering capabilities (by `user`, `configuration`, `comparison_mode`, and `batch_id`) and efficient pagination using datastore cursors. This design is critical for UI responsiveness and scalability, allowing users to browse a potentially vast number of jobs without fetching all data at once. A user experience enhancement is the aliasing of service account emails to "chromeperf (automation)" for better readability in the UI, ensuring that automation-initiated jobs are clearly distinguishable.

*   **`cancel.py` - Job Cancellation:**
    This handler enables users and administrators to terminate running or queued Pinpoint jobs. It incorporates strict authorization checks (`_CheckUser`) to ensure that only the job owner or an authorized administrator can perform a cancellation, including handling delegation for service accounts. Upon successful authorization, the handler invokes the `job.Cancel()` method, which updates the job's internal state to "Cancelled" and posts a descriptive comment to any associated bug, maintaining transparency and communication throughout the job lifecycle. Error conditions for already-cancelled or non-existent jobs are gracefully handled.

*   **`fifo_scheduler.py` - FIFO Scheduling Cron Job:**
    This handler operates as a critical cron job, periodically polling various FIFO queues within the Pinpoint service to initiate queued jobs. The `_ProcessFIFOQueues` function iterates through each configured queue. A key design element is the "budget" system for capacity-aware scheduling (`scheduler.GetSchedulerOptions`), which assigns a "cost" to different job comparison modes (e.g., 'try', 'performance'). This mechanism ensures that system resources are allocated efficiently, prioritizing higher-cost jobs or allowing more lower-cost jobs to run within a defined budget per scheduling cycle. It also enforces that only one job for a given configuration is actively running at any time, preventing resource contention.

*   **`refresh_jobs.py` - Frozen Job Recovery Cron Job:**
    As a recurring cron job, this handler's responsibility is to provide resilience against transient system failures. It identifies "frozen" jobs—those marked as running but which haven't been updated for an extended period (`_JOB_FROZEN_THRESHOLD`). The handler attempts to restart these jobs, employing a retry mechanism (`_JOB_MAX_RETRIES` via `layered_cache`) to prevent infinite retry loops. If a job repeatedly fails to restart, it is ultimately marked as failed. This automatic recovery process ensures that jobs don't get permanently stuck and eventually reach a terminal state, improving overall system robustness and job completion rates. Cloud metrics are published to track the occurrence and handling of frozen jobs.

*   **`task_updates.py` - Asynchronous Task Progress Updates:**
    This handler is a crucial asynchronous integration point, designed to receive status updates from external services like Swarming or Buildbucket via Pub/Sub. The `HandleTaskUpdate` function decodes base64-encoded JSON messages, extracts job and task identifiers, and then leverages Pinpoint's internal `task_module.Evaluate` mechanism with an `evaluator.ExecutionEngine` to update the job's complex state graph. This event-driven architecture allows Pinpoint to react dynamically to external task completions or failures (e.g., a build succeeding, a test failing), advancing the overall job workflow in real-time without busy-waiting. It specifically targets jobs configured to use the "execution engine," indicating a flexible architectural approach to support different execution models or migrations. Upon processing, the job's `updated` timestamp is transactionally recorded, and an "initiate" event is sent to potentially schedule new tasks based on the updated state.

*   **`results2.py` - Results Generation and Retrieval:**
    This handler manages the generation and access of comprehensive "results2" files for completed Pinpoint jobs. When a client requests results for a job, `Results2Handler` first checks if cached results are available. If not, it schedules the asynchronous generation of these results in a background task (`results2.ScheduleResults2Generation` and `results2.GenerateResults2`). This design choice ensures that potentially long-running data aggregation and processing operations do not block API requests, maintaining a responsive user experience. It provides status updates ("job-incomplete", "pending", "complete", "failed") to the client, reflecting the current state of results availability.

*   **`builds.py` - Recent Build Information:**
    This handler provides an API to retrieve information about recent builds for a specified bot configuration. The `RecentBuildsGet` function queries the `buildbucket_service`, using a `BUILDER_MAPPING` to translate user-friendly bot configuration names into the specific builder names understood by Buildbucket. This abstraction simplifies client-side integration by centralizing knowledge of external naming conventions and providing a consistent interface for build lookups.

*   **`commit.py` and `commits.py` - Git Commit Information:**
    *   **`commit.py`:** This handler retrieves details for a single Git commit (by hash and repository). It acts as a wrapper around the `change.Commit.FromDict` method, abstracting the specifics of interacting with the underlying Git service.
    *   **`commits.py`:** This handler fetches a range of Git commits between a specified `start_git_hash` and `end_git_hash`. This functionality is fundamental for Pinpoint's bisection capabilities, allowing the service to identify and analyze the set of code changes relevant to a performance regression or functional change.

*   **`config.py` - Bot Configuration Listing:**
    This handler provides an API to list all available bot configurations that Pinpoint can utilize. The `ConfigHandlerPost` function dynamically retrieves these configurations from `bot_configurations.List()`. This enables client applications, such as the Pinpoint UI, to populate selection menus or validate user input against currently supported test environments, simplifying the process of creating new jobs.

*   **`culprit.py` - Culprit Verification Updates (Sandwich Workflows):**
    This handler is a cron job responsible for updating the status of "sandwich verification" workflows, which are multi-step processes for re-verifying identified culprits. The `CulpritVerificationResultsUpdateHandler` periodically checks the completion status of all `CloudWorkflow` executions within a `SandwichWorkflowGroup`. Once all workflows are complete, it summarizes their findings. Depending on whether the culprit could be reproduced, it updates the associated bug in the issue tracker (e.g., closing it as "WontFix" if no repro, or merging the bug if verified) and marks the workflow group as inactive. This automates the final stages of culprit verification and integrates directly with bug tracking systems for timely updates.

*   **`stats.py` - Aggregated Job Statistics:**
    This handler provides high-level, aggregated statistics about recently completed Pinpoint jobs. The `StatsHandler` queries jobs created within a recent timeframe (`28 days`) and extracts key metrics such as `status`, `comparison_mode`, `created`, `started`, `difference_count`, and `updated`. This data is primarily used for monitoring Pinpoint's overall usage, performance, and job distribution, offering quick insights into system health without exposing granular job details.

*   **`isolate.py` - Isolate Tracking:**
    This handler provides mechanisms to track and retrieve isolate information (build artifacts) linked to specific builders and commits. The `POST` method (`IsolateHandler`) allows for storing these mappings in the datastore, effectively creating a centralized cache for build artifact locations. The `GET` method enables retrieval of the `isolate_server` and `isolate_hash` for a given `builder_name`, `change`, and `target`. This decouples the test execution phase from the intricacies of the build system's artifact storage. An associated cron job, `IsolateCleanupHandler`, handles the periodic removal of expired isolate entries.

*   **`migrate.py` - Data Model Migration Cron Job:**
    This handler is an administrative cron job designed to facilitate schema evolution for Pinpoint's job data models. The `MigrateHandler` allows administrators to trigger a migration process that iterates through existing `job.Job` entities in batches (`_BATCH_SIZE`), applying necessary updates or transformations when the underlying data model changes. It uses datastore cursors for efficient processing of large datasets and maintains progress and error statistics (`_STATUS_KEY`), ensuring that large-scale data migrations are manageable, observable, and resilient to individual entity failures. This is a critical operational utility for maintaining data consistency across software updates.

### Key Workflows

#### New Job Creation and Scheduling

```
User/Service Initiates Job Request
        |
        V
/api/new Handler Receives Request
        |
        V
1. Argument Validation and Normalization
   - Merges default parameters from `bot_configurations`
   - Validates `comparison_mode`, `changes`, `bug_id`, `priority`, `tags`
        |
        V
2. Quest Generation (`_GenerateQuests`)
   - Constructs sequence of operations (e.g., Build, Test, ReadValue) based on job type
        |
        V
3. Job Object Instantiation (`job_module.Job.New`)
   - Persists the initial job state to the datastore
        |
        V
4. Job Scheduling (`scheduler.Schedule`)
   - Adds job to appropriate FIFO queue
   - Calculates and applies job cost for queueing priority
        |
        V
5. Post-Creation Updates (`job.PostCreationUpdate`)
   - Performs asynchronous follow-up actions (e.g., posts comment to associated bug)
        |
        V
Client Receives `jobId` and `jobUrl`
```

#### Job Execution via FIFO Scheduler

```
Cron Trigger: `fifo-scheduler` Handler
        |
        V
1. Identify Active Configurations (`scheduler.AllConfigurations`)
   - Retrieves all currently configured queues
        |
        V
For Each Configuration (Queue):
        |
        +-----> Loop: (Continues until budget exhausted or queue empty)
                |
                V
              Pick Next Job (`scheduler.PickJobs`)
              (Prioritizes jobs based on budget and cost model)
                |
                V
              If Job Found:
                |
                +-----> If Job Status is 'Running' AND Pinpoint Job is 'Completed' or 'Failed':
                |         Complete Job in Scheduler Queue (`scheduler.Complete`)
                |         (Removes job from active `Running` state within scheduler)
                |
                +-----> Else If Job Status is 'Queued':
                |         Start Job (`job.Start`)
                |         (Transitions Pinpoint Job to `Running` state)
                |         (Triggers initial tasks like builds or tests)
                |
                +-----> Else (Job already running in Pinpoint, waiting for external updates):
                          Continue monitoring in next cron cycle
                |
                V
              End Loop
```

#### Asynchronous Task Updates and Job State Progression

```
External Service (Swarming/Buildbucket) Pub/Sub Message
        |
        V
`task_updates.TaskUpdatesHandler` Receives Webhook
        |
        V
1. Message Decoding and Parsing (`HandleTaskUpdate`)
   - Extracts `job_id`, `task_type`, `task_id`, and payload from message
        |
        V
2. Job Retrieval (`job_module.JobFromId`)
   - Loads the Pinpoint job associated with the update
        |
        V
3. Task Graph Evaluation (`task_module.Evaluate` with `evaluator.ExecutionEngine`)
   - Updates the job's internal state graph based on task completion or failure
   - Potentially schedules dependent tasks (e.g., tests after a successful build)
        |
        V
4. Check for Terminal State
   - If the root 'performance_bisection' node reaches 'completed' or 'failed':
     - Update overall Job status (`job.Fail` or `job._Complete`)
     - Record `difference_count` if bisection is complete
        |
        V
5. Job Timestamp Update (`job_module.UpdateTime`)
   - Records the time of the last state change for job monitoring (e.g., by `refresh_jobs`)
        |
        V
6. Initiate Next Steps (if any pending)
   - Triggers `task_module.Evaluate` with an 'initiate' event to activate any newly unblocked tasks
        |
        V
`task_updates` Handler Responds (204 No Content)
```





































# Module: /index

The `/index` module serves as the primary entry point and bootstrap mechanism for the Pinpoint web application. It is responsible for initializing the application's core environment, loading essential frameworks and libraries, and orchestrating the rendering of the main user interface.

### Design Philosophy and Implementation Choices

The architecture of the Pinpoint application, as defined by the `/index` module, demonstrates a clear commitment to modern web development paradigms, focusing on modularity, user experience, and robust data presentation.

*   **Component-Based Architecture (Polymer and Web Components)**: The extensive use of Polymer (`polymer.html`) and Web Components polyfills (`webcomponents-lite.min.js`), alongside custom elements like `index-page.html` and `navigation-bar.html`, signifies a design choice to build the user interface from encapsulated, reusable components. This approach enhances maintainability by localizing concerns, promotes consistency across the application, and allows for easier collaboration among developers working on distinct UI parts. The core `index.html` itself instantiates a single top-level custom element, `<index-page>`, indicating that the application's structure is largely delegated to this primary component.

*   **Single Page Application (SPA) with Client-Side Routing**: The inclusion of Polymer's routing elements (`app-location.html`, `app-route.html`, `iron-pages.html`) is central to the application's architecture as a Single Page Application. This design choice aims to provide a more fluid and responsive user experience by avoiding full page reloads as users navigate. `app-location` observes URL changes, `app-route` maps these URLs to specific application states or views, and `iron-pages` dynamically swaps the visible content based on the active route, ensuring a seamless transition between different sections of the application.

*   **Integrated Google Authentication**: The application leverages `accounts.google.com/gsi/client` for user authentication. This decision streamlines user management by relying on Google's established and secure identity platform, reducing the burden of implementing and maintaining a separate authentication system. It allows users to access Pinpoint using their existing Google accounts, simplifying the login process.

*   **Rich Data Visualization Capabilities**: The integration of D3.js (`d3.v5.min.js`) indicates that Pinpoint is designed to handle and present complex data visually. D3.js was chosen for its powerful capabilities in binding data to the Document Object Model (DOM) and enabling the creation of highly customizable, interactive data visualizations. This underscores the application's need to effectively communicate insights derived from potentially large or intricate datasets.

### Key Components and Responsibilities

The `/index` module's operation is orchestrated through several key files and custom elements, each fulfilling a specific role in the application's lifecycle and presentation.

*   **`/index/index.html`**:
    *   **Responsibility**: This is the application's entry point, acting as the root document that the browser initially loads. Its primary role is to set up the foundational environment by importing all necessary external libraries (Polymer, D3.js, Google Sign-In client) and custom web components. It also defines the basic page styling and title.
    *   **Contribution to Design**: By importing all dependencies centrally, it ensures that the application's core technologies are available before any specific application logic or UI is rendered. Its minimalistic `<body>` content, containing only `<index-page>`, highlights the application's component-driven design where a single root component is responsible for building the entire UI.

*   **`/elements/index-page.html`**:
    *   **Responsibility**: As a custom Polymer element, `index-page.html` serves as the main application container and orchestrator. It is expected to encapsulate the primary application logic, manage the overall layout, integrate the navigation bar, and dynamically render different views based on the current route.
    *   **Contribution to Design**: Its central role as the sole element in the `index.html` body underscores the application's commitment to modularity. It aggregates various sub-components to form the complete application interface, abstracting the complexity of the application structure from the root HTML file.

*   **`/elements/navigation-bar.html`**:
    *   **Responsibility**: This custom Polymer element is dedicated to providing consistent navigation throughout the Pinpoint application. It typically contains the application's logo, user authentication status (e.g., Google Sign-In controls), and links or controls that allow users to access different features or sections of the application.
    *   **Contribution to Design**: By separating navigation into its own reusable component, the application maintains a consistent user experience and simplifies the integration of navigation controls across different parts of the application.

### Application Initialization Workflow

The following pseudographic diagram illustrates the high-level flow when a user accesses the Pinpoint application:

```
User navigates to Pinpoint URL
       |
       v
Browser loads /index/index.html
       |
       +--- Fetches Web Components Polyfills & Polymer framework
       |
       +--- Fetches D3.js library (for data visualization)
       |
       +--- Fetches Google Sign-In client library (for authentication)
       |
       +--- Imports custom Polymer elements (e.g., index-page.html, navigation-bar.html)
       |
       v
<index-page> custom element is instantiated in the DOM
       |
       v
<index-page> constructs the main application layout:
  - Initializes client-side routing using app-location, app-route, iron-pages.
  - Integrates <navigation-bar> for global navigation.
  - Renders initial view based on current URL path.
       |
       v
Application UI is fully rendered and interactive, responding to user actions and URL changes.
```





# Module: /models

The `/models` module forms the foundational data model and business logic core of Pinpoint, orchestrating all aspects of performance bisection and analysis. It defines the core entities, their relationships, and the dynamic processes required to execute complex, multi-repository performance workflows. Its design prioritizes reliability, extensibility, and efficient handling of multi-repository code changes and their associated performance data.

The module solves the challenge of automating performance regression and improvement detection by:
*   **Representing Code States:** Providing a precise, immutable abstraction for "a code change" that can span multiple Git repositories and include unmerged Gerrit patches.
*   **Orchestrating Workflows:** Defining a hierarchical structure for jobs, quests, and executions that allows for modular definition and stateful tracking of distributed work, from building binaries to running tests and parsing results.
*   **Intelligent Exploration:** Implementing sophisticated statistical comparison and search algorithms to efficiently pinpoint culprit changes in large commit ranges.
*   **Dynamic Adaptation:** Enabling the system to dynamically modify its execution plan and data collection strategy in response to intermediate results or inconclusive findings.
*   **Robust Reporting:** Translating complex internal states and analysis results into clear, actionable external communications for users and integrated systems (e.g., issue trackers, Gerrit, BigQuery).

### Core Abstractions and Workflow Management

Pinpoint jobs operate through a layered execution model, progressively breaking down high-level objectives into actionable steps and their concrete executions.

*   **`job.py`**: The `Job` entity is the top-level unit of work. It encapsulates user requests (arguments, bug ID, comparison mode), job metadata (creator, creation time), and its overall status (running, completed, failed, cancelled). `job.py` orchestrates the job's lifecycle, initiating its execution, triggering external service interactions (e.g., posting bug comments, updating Gerrit), and managing job-level failures. For legacy jobs, it manages the `JobState` (PickleProperty), while for newer jobs, it interacts with the execution engine (`task.py`).

*   **`job_state.py`**: This module manages the internal, dynamic state of a Pinpoint `Job`, particularly for jobs using the legacy state management. It holds the list of `Quests` (steps), `Changes` (code revisions being tested), and `Attempts` (runs of quests on a change). Crucially, `job_state.py` contains the core business logic for **bisecting**:
    *   **`Explore()`**: This method drives the adaptive search for culprit changes. It compares the results of adjacent `Changes`. If a difference is detected, it uses the `exploration` module to identify intermediate `Changes` for further testing. If results are inconclusive, it instructs the system to run more `Attempts` to gather additional statistical confidence.
    *   **`ScheduleWork()`**: Iterates through all `Attempts` and calls their `ScheduleWork()` method, ensuring all pending tasks are initiated.
    The module's design, particularly the `PickleProperty` for state storage, allows for complex, mutable Python objects to define job progression, though newer jobs leverage the `task` graph for more granular control.

*   **`attempt.py`**: Represents a single, complete run of all `Quests` on a specific `Change`. An `Attempt` creates an `Execution` object for each `Quest` and runs them serially. If any `Execution` fails, subsequent `Executions` in that `Attempt` are skipped, and the entire `Attempt` is marked as failed. This module is designed to encapsulate the sequential execution and failure propagation logic for a single pass over the defined workflow steps.

### Precise Code Identification: The `change` Module

The `/models/change` module provides the core abstraction for representing specific points in a project's source code history. This is vital for Pinpoint to accurately identify, reproduce, and navigate codebase states, especially in complex, multi-repository environments.

*   **`change.py`**: The `Change` class is the central, immutable representation of a code state. It encapsulates a tuple of `Commit` objects (for multiple repositories) and an optional `GerritPatch`. Its immutability ensures reliable reproducibility for bisecting.
    *   **Design Rationale**: A single Git hash is insufficient for projects like Chromium, which rely on coordinated changes across many repositories via `DEPS` files. `Change` provides a composite view.
    *   **`Midpoint()`**: This critical method implements the logic for finding an intermediate code state between two `Changes`. It intelligently handles `DEPS` rolls across different repositories, parsing `DEPS` files (via `commit.Deps()`) to align dependencies before calculating a midpoint. This is fundamental to Pinpoint's bisecting capabilities.

*   **`commit.py`**: Represents a single Git commit within a specific repository. It identifies the repository and Git hash, resolves Gitiles URLs, and fetches detailed metadata (author, message) from Gitiles, which is then cached.
    *   **`Deps()`**: This method is crucial for multi-repository context. It parses the `DEPS` file at this specific commit to discover the full set of dependencies, enabling `Change.Midpoint` to understand the complete "world state".

*   **`patch.py`**: Encapsulates a Gerrit code review patch. It stores the Gerrit server, change ID, and revision, and provides `BuildParameters` to integrate with external build systems (like Buildbucket) for testing unmerged changes.

*   **`repository.py`**: Abstracts away the specifics of Gitiles URLs by mapping short, user-friendly repository names (e.g., 'chromium') to their canonical URLs. This centralizes URL management and provides flexibility.

*   **`commit_cache.py`**: A memcache-only module dedicated to caching expensive-to-fetch metadata for `Commit` and `GerritPatch` objects. Its "memcache-only" design prevents Datastore overhead for transient data, optimizing performance by reducing external API calls to Gitiles and Gerrit.

**Key Workflow: Bisecting a Change Range**

```
Change A                                        Change B
(start of range)                                (end of range)
        |                                               |
        V                                               V
change.Change.Midpoint(Change A, Change B)
        |
        +-- Check for patch compatibility; if incompatible, Raise NonLinearError
        |
        +-- _ExpandDepsToMatchRepositories(A.commits, B.commits)
        |   (Ensures commit lists include all relevant repos by parsing DEPS)
        |       |
        |       +-- For each Commit: commit.Commit.Deps()
        |       |   (Fetches DEPS file from Gitiles, parses via gclient_eval)
        |       V
        |   Mutates commit lists in-place to align dependencies
        |
        +-- _FindMidpoints(aligned_commits_A, aligned_commits_B)
        |       |
        |       +-- For each (commit_a, commit_b) pair:
        |       |       commit.Commit.Midpoint(commit_a, commit_b)
        |       |             |
        |       |             V
        |       |       gitiles_service.CommitRange(repo_url, hash_a, hash_b)
        |       |           (Finds midpoint within a single repo's history)
        |       |
        |       +-- If adjacent commits have DEPS changes:
        |       |       Re-evaluate and expand dependency lists
        |       V
        |   Returns list of midpoint Commits
        |
        V
New Change Object (representing the midpoint, with original patch)
```

### Flexible Work Execution: The `task` and `evaluators` Modules

These modules introduce Pinpoint's event-driven execution engine, providing a more granular and flexible approach to managing job workflows compared to the legacy `JobState` model.

*   **`task.py`**: Defines the fundamental building blocks for the execution engine:
    *   `Task`: A Datastore entity representing a single unit of work within a job, with a type, status (`pending`, `ongoing`, `completed`, `failed`, `cancelled`), and payload. Task updates are transactional and enforce valid state transitions.
    *   `TaskVertex`, `Dependency`, `TaskGraph`: Namedtuples used to define the structure of the task graph, representing tasks and their inter-dependencies.
    *   **`Evaluate()`**: This is the core of the execution engine. It performs a depth-first traversal of the task graph, calling an `evaluator` function for each task. It's event-driven, meaning evaluators process `Event` objects (`event.py`) to trigger state changes or schedule new actions, and dynamically extends the graph if necessary. This decouples task logic from traversal mechanics and allows for iterative refinement of the workflow.
    *   **Why a new engine?**: The explicit task graph and event-driven model offer greater flexibility and scalability over `JobState`'s `PickleProperty`, enabling more complex, dynamic workflows, clearer separation of concerns, and robust error handling.

*   **`event.py`**: Defines the `Event` namedtuple, serving as the canonical input to the `task.Evaluate()` function. Events (`'initiate'`, `'update'`, `'select'`) drive the task graph processing, triggering evaluators to react to external stimuli or internal state changes.

*   **`/models/evaluators`**: This package provides a highly composable framework for defining the evaluation logic that `task.Evaluate()` uses.
    *   **Design Rationale**: Instead of monolithic `if/else` statements, `evaluators` promotes small, reusable `Filters` (predicates like `TaskTypeEq`) and `Evaluators` (actions like `TaskPayloadLiftingEvaluator`) that can be combined declaratively using combinators (`SequenceEvaluator`, `FilteringEvaluator`). This makes evaluation logic easier to reason about, maintain, and extend.
    *   `DispatchByTaskType`, `DispatchByEventTypeEvaluator`: These are key components that dynamically route incoming tasks and events to the appropriate, specialized evaluator based on their type or event. This is central to how the `ExecutionEngine` selects the correct logic for a `find_isolate` task versus a `run_test` task.
    *   `job_serializer.py`: A specialized `Evaluator` that transforms the internal, granular `TaskGraph` state into a structured, human-readable dictionary suitable for external consumption (e.g., the Pinpoint Web UI, APIs). It aggregates and reorders task data, ensures consistent formatting, and extracts high-level job parameters.

**Key Workflow: Job Serialization (from `evaluators/job_serializer.py`)**

```
+----------------+       +------------------------------------+
| Incoming Task  | ----> | job_serializer.Serializer.__call__ |
|  & Event       |       +------------------------------------+
+----------------+                 | (Delegates based on Task Type)
                                   |
                                   v
+--------------------------+  +-------------------------------+
| SequenceEvaluator for    |  | SequenceEvaluator for         |
| 'find_isolate', 'run_test',|  | 'find_culprit' tasks          |
| 'read_value' tasks       |  |                               |
+--------------------------+  +-------------------------------+
       |                      |
       v                      v
+------------------------+  +------------------------+
| Task-specific Serializer|  | performance_bisection. |
| (e.g., find_isolate.S) |  | Serializer             |
+------------------------+  +------------------------+
       |                      |
       v                      v
+------------------------+  +------------------------+
| TaskPayloadLiftingEvaluator |  | AnalysisTransformer   |
+------------------------+  +------------------------+
       |                      |
       v                      v
+--------------------------+  +--------------------------+
|  TaskTransformer         |  | Local Context for Culprit|
| (Local Context for Tasks)|  |  Data                    |
+--------------------------+  +--------------------------+
       |                      |
       |  (Returned to Serializer)
       +--------------------------------------------------+
                                   | (Serializer merges local context into global)
                                   v
+----------------------------------------------------------------------------------+
| Global Job Context (Accumulator)                                                 |
| (Updated with task states, ordered changes, comparisons, and job parameters)     |
+----------------------------------------------------------------------------------+
```

### Intelligent Search for Performance Changes: Statistical Comparison and Exploration

These modules provide the analytical "brain" for Pinpoint, enabling it to detect performance shifts and efficiently search for their root causes.

*   **`/models/compare`**: This module offers a robust statistical framework for comparing two sets of sampled data, outputting one of three decisions: `DIFFERENT`, `SAME`, or `UNKNOWN`.
    *   **Design Rationale**: A single statistical test is often insufficient for noisy performance data. This module uses both the Kolmogorov-Smirnov (K-S) test (sensitive to distribution shape/variance) and the Mann-Whitney U (MWU) test (sensitive to central tendency shifts). By taking the minimum p-value, it leverages the strongest evidence for a difference.
    *   **Three-Way Decision Logic**: Crucially, it uses two significance thresholds (`low_threshold` and `high_threshold`). If the p-value is below `low_threshold`, results are `DIFFERENT`. If above `high_threshold`, they are `SAME`. If between, results are `UNKNOWN`, signaling that more data is needed. This prevents premature conclusions and guides adaptive data collection.
    *   `kolmogorov_smirnov.py`, `mann_whitney_u.py`: Provide pure Python implementations of these statistical tests.
    *   `thresholds.py`: Manages dynamic `high_threshold` values based on comparison `mode`, expected `magnitude` of change, and `sample_size`. These are pre-computed through extensive simulations (in `thresholds_functional.py` and `thresholds_performance.py`) to ensure statistical rigor in the `UNKNOWN` decision.

*   **`exploration.py`**: Implements the `Speculate()` function, central to Pinpoint's binary search algorithm for finding culprits. Given a range of `Changes` and a function to detect differences, `Speculate()` performs a binary infix traversal, generating new intermediate `Changes` for exploration.
    *   **Design Rationale**: Bisection allows for rapid narrowing of a search space. `Speculate()` enables looking "levels ahead" in the bisection tree, trading increased computational resources (testing more points concurrently) for a reduced overall time-to-culprit.

### Orchestrating External Services: Concrete `Quest` Implementations

The `/models/quest` module defines the abstract blueprint (`Quest`) and stateful instance (`Execution`) for every unit of work performed by Pinpoint. This structure enables modular interaction with diverse external services and internal logic.

*   **`quest.py`**: Defines the abstract `Quest` base class. A `Quest` is a declarative definition of a work unit (e.g., "Build Chromium"). It provides a `Start()` method to initiate an `Execution` and a `PropagateJob()` method to provide job context. The `FromDict()` class method is a factory for creating `Quest` instances from configuration dictionaries.

*   **`execution.py`**: Defines the abstract `Execution` base class. An `Execution` is a stateful instance of a `Quest` running for a specific `Change`. It tracks `_completed` status, `_exception` details, `_result_values` (metrics), and `_result_arguments` (data passed to subsequent executions).
    *   **`Poll()`**: This critical method is periodically called to advance the execution's state. It includes robust error handling to distinguish between fatal `JobError` exceptions (which halt the entire job) and execution-specific errors (which only fail that execution, potentially allowing retries or other parts of the job to proceed).

*   **`find_isolate.py`**: Implements the `FindIsolate` `Quest` and its `_FindIsolateExecution`. This task is responsible for obtaining a build artifact (an isolate hash or CAS root reference) for a given `Change`.
    *   **How it works**: It first checks a cache for a pre-existing isolate. If not found, it interacts with Buildbucket to trigger a new build. `_FindIsolateExecution` then polls Buildbucket for build status, extracts the artifact reference upon success, and passes it as `result_arguments` to the next quest. This abstracts away build system complexities.

*   **`run_test.py`**: Serves as the base `Quest` for running any command or test on Swarming bots. It handles the generic logic for constructing Swarming task requests (dimensions, command, tags), scheduling tasks, polling their status, and interpreting outcomes.
    *   **How it works**: It takes isolate/CAS references from previous quests. Its `_RunTestExecution` monitors Swarming task states (`PENDING`, `RUNNING`, `COMPLETED`, `EXPIRED`, `FAILURE`) and extracts the output isolate/CAS root reference. It also reports Swarming job metrics.

*   **`read_value.py`**: Implements the `ReadValue` `Quest` and its `ReadValueExecution`. Its responsibility is to extract specific numerical performance metrics from the raw output files (e.g., `perf_results.json`) generated by completed test runs.
    *   **How it works**: It retrieves the output file from an isolate server or CAS, then parses it (supporting legacy GraphJSON and modern HistogramSet formats). It extracts target metrics using filters (metric name, story, statistic) and handles cases where data is missing or malformed, raising specific `ReadValue` errors. The extracted values are stored as `result_values`.

*   **Specialized `run_*.py` Quests**: Modules like `run_browser_test.py`, `run_gtest.py`, `run_telemetry_test.py`, etc., extend `run_test.py`. They provide specific implementations tailored for different testing frameworks and environments within Chromium and related projects. They override methods like `_ComputeCommand` and `_ExtraTestArgs` to configure precise command lines and arguments for their respective test types, abstracting away the invocation details for each test system.

**Key Workflow: General Pinpoint Job Flow (Quest and Execution Lifecycle)**

```
                  Job Starts
                     |
                     v
          +--------------------+
          |   Quest 1 (Build)  |  (e.g., FindIsolate)
          +--------------------+
                | Start(change)
                v
          +--------------------+    Poll() calls _Poll() until complete
          | Execution 1 (Build)| <-------------------------------+
          | - Request Build    |                                 |
          | - Check Build Status |                               |
          +--------------------+                                 |
                | _Complete()                                   |
                | (result_arguments: isolate_hash or cas_root_ref)
                v                                                 |
          +--------------------+                                 |
          |   Quest 2 (Test)   |  (e.g., RunTelemetryTest)
          +--------------------+                                 |
                | Start(change, isolate_hash/cas_root_ref)         |
                v                                                 |
          +--------------------+                                 |
          | Execution 2 (Test) | <-------------------------------+
          | - Start Swarming Task|                               |
          | - Poll Swarming Status|                               |
          +--------------------+                                 |
                | _Complete()                                   |
                | (result_arguments: cas_root_ref or isolate_hash)
                v                                                 |
          +--------------------+                                 |
          |  Quest 3 (Read Value)| (e.g., ReadValue)
          +--------------------+                                 |
                | Start(change, cas_root_ref/isolate_hash)         |
                v                                                 |
          +--------------------+                                 |
          | Execution 3 (Read Value)| <---------------------------+
          | - Retrieve Output from CAS/Isolate |
          | - Parse Metrics        |
          +--------------------+
                | _Complete()
                | (result_values: [metric_value])
                v
                  Job Finished
```

### Automated Culprit Identification: `performance_bisection.py`

This module contains the core logic for the automated performance bisection algorithm. It dynamically constructs and refines the task graph to pinpoint the specific commit(s) responsible for a performance change.

*   **Design Rationale**: Manually bisecting large commit ranges is laborious and error-prone. This module automates the entire process, iteratively testing intermediate commits, statistically comparing results, and narrowing the search space.
*   **How it works**: A `find_culprit` task (created by `CreateGraph`) first fetches all commits in the specified range (`PrepareCommits`). It then initiates `read_value` tasks for the start and end commits (and dynamically added intermediate commits). The `FindCulprit` evaluator processes results from its `read_value` dependencies, uses `compare.Compare` for statistical comparisons, and `exploration.Speculate` to determine the next steps. This involves:
    *   Identifying `culprits` if a significant difference is localized.
    *   Increasing the number of attempts for existing `read_value` tasks (via `RefineExplorationAction`) if comparisons are `UNKNOWN`.
    *   Adding new `read_value` tasks for intermediate commits (also via `RefineExplorationAction`) to further narrow the search space.
This iterative process continues until culprits are found, or no significant change is reproducible.

### Job Lifecycle Management and Reporting

This collection of modules ensures that Pinpoint jobs are efficiently scheduled, their progress and outcomes are clearly communicated, and their historical data is preserved.

*   **`scheduler.py`**: Implements a simple FIFO scheduler for Pinpoint jobs, organized into `ConfigurationQueue` instances (one per builder configuration).
    *   **How it works**: Jobs are enqueued into their respective configuration queues. `PickJobs()` selects jobs for execution based on priority and submission time, respecting a configurable `budget` (resource limit). It also tracks `SampleElementTiming` to provide insights into queueing delays. `Schedule()`, `Cancel()`, and `Complete()` manage a job's status within the queue.

*   **`job_bug_update.py`**: Formats and posts updates to external issue trackers (like Monorail) in response to job events.
    *   **Design Rationale**: Standardizes communication with issue trackers, automatically providing critical context.
    *   **How it works**: It uses Jinja2 templates (`differences_found.j2`, `job_created.j2`, `missing_values.j2`) to render structured comments. It dynamically assigns owners (e.g., to the author of the culprit commit, or a sheriff for autorolls), applies relevant labels (`Pinpoint-Culprit-Found`, `Pinpoint-No-Repro`), and can merge issues if a duplicate culprit is found.

*   **`results2.py`**: Manages the generation and storage of user-facing job results.
    *   **How it works**: It creates an interactive `results2.html` page (by rendering collected histograms into a `vulcanized_histograms_viewer.html` template) and stores it in Google Cloud Storage. It also exports detailed job and performance metric data to BigQuery for long-term analytics and dashboarding, enabling deep analysis of trends.

*   **`timing_record.py`**: Records historical execution times for completed Pinpoint jobs and provides runtime estimates for new jobs.
    *   **How it works**: It stores `TimingRecord` entities with job duration and associated tags (comparison mode, configuration, benchmark, story). When a new job is created, `GetSimilarHistoricalTimings()` queries for past jobs with matching tags to provide an estimated completion time based on median, standard deviation, and 90th percentile of similar runs.

*   **`sandwich_workflow_group.py`**: Manages the state and progress of automated culprit verification workflows, often referred to as "sandwich" builds.
    *   **How it works**: When a regression is detected by Pinpoint, this module can initiate external workflow executions (e.g., Cloud Workflows) to perform further verification. It stores `CloudWorkflow` entities, tracking their status, associated commits, and the results of these verification runs, linking them back to the original `bug_id`.

### Error Handling

*   **`errors.py`**: Defines a comprehensive hierarchy of custom exception classes (`JobError`, `FatalError`, `InformationalError`, `RecoverableError`).
    *   **Design Rationale**: This structured approach allows Pinpoint to categorize failures (e.g., build errors, test failures, Pinpoint internal errors, user request issues) and respond appropriately. Fatal errors halt the job, while recoverable errors might trigger retries. This provides clearer, more actionable feedback to users and aids internal debugging.






# Module: /models/change

The `/models/change` module serves as the core abstraction layer for representing and manipulating specific points in a project's source code history, vital for tools like Pinpoint (a performance bisecting system). Its primary responsibility is to define a "Change" as a composite of various Git commits across multiple repositories, optionally augmented with a Gerrit code review patch. This comprehensive definition allows Pinpoint to precisely identify and reproduce a particular state of the source tree for testing and analysis.

The module focuses on ensuring that these code states are accurately described, can be serialized for storage or network transfer, and can be intelligently navigated for bisecting purposes, especially in complex, multi-repository environments.

### Key Components and Their Responsibilities

The module is structured around several distinct classes, each handling a specific aspect of defining and interacting with a code change:

#### `change.py` (The `Change` Class)

The `Change` class is the central entity, representing a complete snapshot of the codebase at a given point. It's designed as an immutable `collections.namedtuple`, a deliberate choice to ensure that once a Change is defined, its underlying components (commits, patch) cannot be altered. This immutability is crucial for the reliability and reproducibility of bisecting operations, where consistent comparisons between distinct code states are paramount.

*   **Composition of a Code State:** A `Change` is fundamentally composed of a tuple of `Commit` objects and an optional `GerritPatch`. This multi-faceted approach acknowledges that a "change" in a large project like Chromium often involves coordinated updates across several repositories (e.g., `chromium/src`, `v8`, `catapult`) and may include an unmerged code review.
*   **Identification and Representation:** Methods like `__str__` provide human-readable summaries, while `id_string` generates a stable, order-independent identifier for a Change. The `id_string` is particularly useful for consistent lookup and caching of build results, regardless of the order in which individual commits were specified.
*   **Updating Changes:** The `Update` method demonstrates a key design pattern for immutable objects. Instead of modifying an existing `Change` in place, it returns a *new* `Change` object that incorporates overrides or additions from another `Change`. This ensures that historical `Change` references remain valid.
*   **Serialization and Deserialization:** `AsDict`, `FromData`, `FromUrl`, and `FromDict` methods provide robust mechanisms for converting `Change` objects to and from dictionary representations and URLs. This is essential for storing Change information in databases, passing it between services via APIs, and accepting flexible user inputs (e.g., a Gitiles URL for a commit, or a Gerrit URL for a patch).
*   **Midpoint Calculation for Bisecting:** The `Midpoint` class method is central to the bisecting algorithm. Its responsibility is to find a code state roughly "halfway" between two given `Change` objects. This is a complex task due to multi-repository dependencies:
    *   **Handling `DEPS` Rolls:** Projects often manage dependencies through `DEPS` files (used by `gclient`). A change in the primary repository might trigger a "roll" of a dependency to a new version. `Midpoint` intelligently handles these `DEPS` rolls across different repositories. It uses private helper functions (`_ExpandDepsToMatchRepositories`, `_FindMidpoints`) that parse `DEPS` files (via `commit.Deps()`) to ensure that all relevant repositories are considered and aligned before finding the midpoint.
    *   **Non-Linearity Detection:** `Midpoint` raises a `NonLinearError` if the two `Change` objects are incompatible for bisecting (e.g., they have different patches, non-matching repositories, or an illogical commit order). This prevents the bisecting logic from attempting to find a midpoint in an ambiguous or impossible scenario.

#### `commit.py` (The `Commit` Class)

The `Commit` class represents a single, specific Git commit within a single repository. It's the atomic unit of version control that `Change` objects compose.

*   **Repository and Hash Identification:** Each `Commit` primarily identifies a repository (using a short name like 'chromium') and a `git_hash`. It also dynamically resolves the full Gitiles URL for the repository (`repository_url`) to interact with Gitiles services.
*   **Dependency Resolution:** The `Deps` method is crucial. It parses the `DEPS` file located at this specific commit within its repository. This is how the system discovers the full set of dependencies (other repositories at specific commits) that define the "world state" for the `Change` at this `Commit`. It leverages `depot_tools.gclient_eval` for accurate parsing of these files and raises a `DepsParsingError` if the file is malformed.
*   **Commit Metadata and Caching:** `Commit` objects are responsible for fetching detailed metadata about themselves (author, commit time, subject, message) from Gitiles. To optimize performance and reduce external API calls, this metadata is aggressively cached using the `commit_cache` module (`GetOrCacheCommitInfo`, `CacheCommitInfo`).
*   **Commit Creation from Various Sources:** Similar to `Change`, `Commit` provides `FromData`, `FromUrl`, and `FromDict` methods to instantiate `Commit` objects from diverse inputs (Gitiles URLs, dictionaries). It includes logic to resolve symbolic references like 'HEAD' or commit positions to concrete Git hashes, making it robust against different input formats.
*   **Single-Repository Midpoint:** The `Midpoint` class method for `Commit` focuses on finding a midpoint within a single repository's linear history using `gitiles_service.CommitRange`. It handles caching of intermediate commit details and raises a `NonLinearError` if commits are in different repositories or not in a chronological sequence, ensuring the linearity required for bisecting.

#### `patch.py` (The `GerritPatch` Class)

The `GerritPatch` class encapsulates information about a Gerrit code review patch that can be applied on top of a base set of commits. This allows performance testing of unmerged changes.

*   **Canonical Representation:** It stores the Gerrit `server` URL, the canonical `change` ID (e.g., `project~branch~Change-Id`), and the specific `revision` (patch set ID or commit hash). This canonical form ensures consistent identification and lookup of patches.
*   **Integration with Build Systems:** Key methods like `BuildParameters` and `BuildsetTags` generate the necessary data for external build systems (e.g., Buildbucket) to correctly fetch and apply the patch. This is critical for orchestrating the actual testing of the patched code.
*   **Metadata Retrieval and Caching:** The `AsDict` method fetches detailed patch information (author, creation time, subject, message) from Gerrit via `gerrit_service`. Like `Commit` metadata, this information is cached using `commit_cache` to improve performance.
*   **Flexible Patch Specification:** `FromData`, `FromUrl`, and `FromDict` methods enable the creation of `GerritPatch` objects from various user inputs, including different Gerrit URL formats and dictionary representations. It includes logic to resolve unspecified revisions to the `current_revision` of a change, simplifying user input.

#### `repository.py`

This module provides a mapping service between user-friendly short repository names (e.g., 'chromium', 'v8') and their canonical Gitiles URLs (e.g., `https://chromium.googlesource.com/chromium/src`).

*   **Abstraction for Repository Paths:** By centralizing repository URL management, `repository.py` abstracts away the specifics of Gitiles paths. This means users can specify 'chromium' instead of its full URL, and the system can handle updates to repository URLs without requiring changes across the entire codebase.
*   **Dynamic Mapping:** `RepositoryName(url, add_if_missing=False)` and `RepositoryUrl(name)` allow flexible lookup. The `add_if_missing` parameter in `RepositoryName` enables dynamic addition of new repository mappings, which is useful when encountering new dependencies in `DEPS` files.
*   **Persistence and Caching:** The mappings are stored using an `ndb.Model` (`Repository`) and heavily cached in memcache. This ensures that repository name-to-URL resolutions are fast and consistent.

#### `commit_cache.py`

This module provides a dedicated, shared caching mechanism for retrieving expensive-to-fetch metadata (URL, author, created time, subject, message) for both `Commit` and `GerritPatch` objects.

*   **Performance Optimization:** The primary "why" is performance. Fetching detailed commit and patch information from external services like Gitiles and Gerrit involves network calls, which are slow and can be subject to rate limits. Caching significantly reduces the latency and load on these external services.
*   **Memcache-Only Storage:** The `Commit` `ndb.Model` (despite its name, used for both commits and patches) is explicitly configured to be *memcache-only* (`_use_datastore = False`, `_use_memcache = True`). This design choice leverages App Engine's highly optimized in-memory caching layer without incurring the overhead of datastore writes or reads for transient data. Cached entries are given a generous expiry (30 days by default), reflecting that commit/patch metadata changes very infrequently if at all.
*   **Simplified Interface:** It exposes simple `Get(id_string)` and `Put(id_string, ...)` methods, providing a straightforward interface for components to store and retrieve metadata.

### Key Workflows and Processes

This section illustrates how the components interact in critical operations.

1.  **Change Initialization and Metadata Resolution:**
    ```
    User Input (e.g., URL or Dict representing a Change)
            |
            V
    change.Change.FromData(input_data)
            |
            +-- Parses input to identify Commits and/or a GerritPatch
            |
            +-- For each Commit:
            |       commit.Commit.FromData(commit_input)
            |             |
            |             +-- repository.RepositoryName(repo_url, add_if_missing=True)
            |             |   (Maps full URL to short name, caches if new)
            |             V
            |       gitiles_service.CommitInfo(repo_url, git_hash)
            |             |
            |             V
            |       commit_cache.Put(id_string, ...)
            |           (Caches fetched commit metadata)
            |
            +-- For GerritPatch (if present):
            |       patch.GerritPatch.FromData(patch_input)
            |             |
            |             V
            |       gerrit_service.GetChange(server, change_id, ...)
            |             |
            |             V
            |       commit_cache.Put(id_string, ...)
            |           (Caches fetched patch metadata)
            |
            V
    Fully Initialized Change Object (with all metadata cached)
    ```

2.  **Bisecting a Change Range (Finding the Midpoint):**
    ```
    Change A                                        Change B
    (start of range)                                (end of range)
            |                                               |
            V                                               V
    change.Change.Midpoint(Change A, Change B)
            |
            +-- Check for patch compatibility (A.patch == B.patch); if not, Raise NonLinearError
            |
            +-- _ExpandDepsToMatchRepositories(A.commits, B.commits)
            |   (Ensures commit lists include all relevant repositories by parsing DEPS)
            |       |
            |       +-- For each Commit: commit.Commit.Deps()
            |       |   (Fetches DEPS file from Gitiles, parses via gclient_eval)
            |       V
            |   Mutates commit lists in-place to align dependencies
            |
            +-- _FindMidpoints(aligned_commits_A, aligned_commits_B)
            |       |
            |       +-- For each (commit_a, commit_b) pair:
            |       |       commit.Commit.Midpoint(commit_a, commit_b)
            |       |             |
            |       |             V
            |       |       gitiles_service.CommitRange(repo_url, hash_a, hash_b)
            |       |             |
            |       |             V
            |       |       deferred.defer(_CacheCommitDetails, ...)
            |       |           (Background task to cache intermediate commit details)
            |       |
            |       +-- If adjacent commits have DEPS changes:
            |       |       Re-evaluate and expand dependency lists
            |       V
            |   Returns list of midpoint Commits
            |
            V
    New Change Object (representing the midpoint, with original patch)
    ```














# Module: /models/compare

The `/models/compare` module provides a robust statistical framework for determining if two sets of sampled data are statistically "same" or "different", or if more data is needed to make a definitive judgment. This capability is critical for automated performance monitoring and functional regression detection, where precise and reliable comparison is essential to identify actual changes without excessive noise or false positives.

### Design Philosophy and Rationale

The module's design centers on a multi-faceted approach to hypothesis testing, addressing common challenges in analyzing noisy performance or functional data:

1.  **Comprehensive Statistical Power**: Instead of relying on a single statistical test, the module employs two distinct non-parametric tests: the Kolmogorov-Smirnov (K-S) test and the Mann-Whitney U (MWU) test. This is a deliberate choice to enhance the ability to detect various types of changes:
    *   The **Mann-Whitney U test** is particularly effective at detecting shifts in the central tendency (e.g., mean or median) of distributions, making it good for identifying performance regressions or improvements.
    *   The **Kolmogorov-Smirnov test** excels at detecting differences in the overall shape or variance of distributions. This is valuable because changes in performance might manifest as increased variance or a skewed distribution, rather than a simple shift in the mean.
    *   By taking the minimum of the p-values from both tests (`p_value = min(kolmogorov_p_value, mann_p_value)`), the system leverages the "strongest" evidence of a difference. If either test suggests a significant difference, that signal is considered.

2.  **Three-Way Decision Logic (DIFFERENT, SAME, UNKNOWN)**: Traditional hypothesis testing typically yields a binary outcome (reject or fail to reject the null hypothesis). This module introduces a "three-way" decision by defining two significance thresholds: a `low_threshold` and a `high_threshold`.
    *   **Low Threshold**: This functions as a traditional significance level (e.g., alpha = 0.01). If the calculated p-value is below this, the samples are considered `DIFFERENT`, indicating a high confidence in a real change.
    *   **High Threshold**: This threshold addresses situations where the evidence for "same" or "different" is inconclusive. If the p-value falls between the `low_threshold` and `high_threshold`, the result is `UNKNOWN`. This signals that more data collection is required to achieve sufficient statistical power to distinguish between the distributions. This prevents premature conclusions, especially with limited sample sizes.
    *   If the p-value is above the `high_threshold`, the samples are considered `SAME`, indicating high confidence that there is no meaningful difference of the `magnitude` being sought.

3.  **Adaptive Thresholds**: The `high_threshold` is not a fixed value. It dynamically adapts based on the `mode` of comparison ('functional' or 'performance'), the `magnitude` of the difference expected, and the `sample_size`. Smaller expected differences or smaller sample sizes require a higher `high_threshold` (i.e., more stringent conditions to declare "same" or "different"), reflecting the increased uncertainty. This adaptation ensures that the comparison system is sensitive to the context of the data being analyzed.

4.  **Pure Python Implementations**: The statistical tests (K-S and MWU) are implemented in pure Python. This likely ensures minimal external dependencies for runtime operations, which can be crucial in certain deployment environments or to reduce build complexity. The generation of the `high_threshold` tables, however, does leverage the `scipy` library for statistical robustness in the simulation process, highlighting a distinction between runtime dependencies and development-time tools.

### Key Components and Responsibilities

The `/models/compare` module is structured around its core comparison logic and supporting statistical utilities.

*   **`compare.py`**:
    This is the module's primary public interface. It exposes the `Compare` function, which orchestrates the entire comparison process. Its responsibilities include:
    *   Receiving two data samples (`values_a`, `values_b`), along with contextual parameters like `mode` (e.g., 'functional', 'performance'), `magnitude` (expected size of difference), and `attempt_count` (average number of data points per sample).
    *   Retrieving the appropriate `low_threshold` and `high_threshold` from the `thresholds` submodule based on the comparison parameters.
    *   Invoking the Kolmogorov-Smirnov and Mann-Whitney U tests on the input data.
    *   Consolidating the p-values from these tests by taking their minimum.
    *   Applying the three-way decision logic (DIFFERENT, SAME, UNKNOWN) based on the consolidated p-value and the `low_threshold` and `high_threshold`.
    *   Returning a `ComparisonResults` object, which encapsulates the final result, the consolidated p-value, and the thresholds used.

*   **`kolmogorov_smirnov.py`**:
    This file provides a pure Python implementation of the two-sample Kolmogorov-Smirnov test. Its sole responsibility is to compute the K-S test's p-value for two given lists of values. It's designed to assess whether two independent samples are drawn from the same continuous distribution, being particularly sensitive to differences in the shape and spread of the distributions.

*   **`mann_whitney_u.py`**:
    This file contains a pure Python implementation of the Mann-Whitney U rank test. Its responsibility is to calculate the p-value for this non-parametric test. The Mann-Whitney U test is used to determine if two independent samples come from populations with different median values, making it well-suited for detecting shifts in central tendency. The implementation uses a normal approximation for larger sample sizes, which is suitable for many performance analysis scenarios.

*   **`thresholds.py`**:
    This submodule is responsible for providing the dynamic significance thresholds used by the `Compare` function.
    *   It defines `LowThreshold()` as a fixed, universal significance level (0.01).
    *   It implements `HighThreshold(mode, magnitude, sample_size)` which calculates the upper p-value boundary. This calculation uses extensive pre-computed lookup tables (`_HIGH_THRESHOLDS_FUNCTIONAL` and `_HIGH_THRESHOLDS_PERFORMANCE`). These tables are generated offline by helper scripts that simulate numerous comparisons under various conditions, ensuring that the `high_threshold` effectively balances the risk of false negatives with the cost of collecting more data. This is crucial for managing the "UNKNOWN" state and limiting the number of experimental repeats.

*   **`thresholds_functional.py` and `thresholds_performance.py`**:
    These are auxiliary scripts, not part of the runtime comparison logic, but fundamental to the system's design. They are responsible for generating the hardcoded `_HIGH_THRESHOLDS_FUNCTIONAL` and `_HIGH_THRESHOLDS_PERFORMANCE` data found in `thresholds.py`. They do this by running extensive statistical simulations (using `scipy` for accuracy) under various scenarios of `magnitude` and `sample_size` for both functional and performance comparison modes. The output of these scripts is then embedded directly into `thresholds.py` as constants.

### Key Workflow

The core workflow revolves around the `Compare` function's execution to classify the relationship between two sets of observed values (`values_a` and `values_b`).

```
1.  Initialize Comparison
    Input: values_a, values_b, mode, magnitude, attempt_count
    Check for empty samples: If either is empty, return UNKNOWN.

2.  Determine Decision Boundaries
    -   Fetch fixed LowThreshold (alpha for rejecting null hypothesis)
    -   Calculate HighThreshold dynamically based on mode, magnitude, sample_size

3.  Perform Statistical Tests
    -   Compute Kolmogorov-Smirnov p-value (KS_p_value)
    -   Compute Mann-Whitney U p-value (MWU_p_value)

4.  Consolidate Evidence
    -   p_value = min(KS_p_value, MWU_p_value)
        (Select the strongest signal of difference)

5.  Make Decision
    IF p_value <= LowThreshold:
        Result = DIFFERENT
    ELSE IF p_value <= HighThreshold:
        Result = UNKNOWN (suggests more data is needed)
    ELSE (p_value > HighThreshold):
        Result = SAME

6.  Output
    Return ComparisonResults (result, p_value, LowThreshold, HighThreshold)
```















# Module: /models/evaluators

The `/models/evaluators` module is central to how Pinpoint processes and interprets the state of ongoing jobs. It provides a flexible and extensible framework for defining evaluation logic for tasks and events, and for transforming internal job state into a consistent, consumable format for external interfaces, such as the Pinpoint Web UI.

The module's design heavily favors composition over hard-coded logic. Instead of monolithic functions, it offers a collection of reusable "filters" and "evaluators" that can be combined declaratively to build complex evaluation workflows. This approach allows for clear separation of concerns, where filters handle conditional checks (the "if"), and evaluators define actions or transformations (the "then").

### Core Design Principles and Implementation Choices

1.  **Declarative Composition**: The primary design goal is to allow developers to define sophisticated evaluation logic by combining smaller, atomic components. This is achieved through:
    *   **Filters**: Predicates that determine whether specific evaluation logic should be applied. They act as the "conditional" part of an evaluation step.
    *   **Evaluators**: Callable objects that perform actions, modify state, or generate subsequent actions. They represent the "action" part.
    This pattern ensures that evaluation logic is easy to read, reason about, and extend without altering existing components.

2.  **Dynamic Dispatch**: Pinpoint jobs involve various task types and statuses. The module provides mechanisms to dynamically select the appropriate evaluation logic based on these attributes. This prevents the need for large, complex `if/else` or `switch` statements, making the system more maintainable and scalable as new task types are introduced.

3.  **State Transformation and Aggregation**: As tasks execute, they produce data that needs to be collected and presented coherently. The `evaluators` module includes specific components for:
    *   **Lifting Payload**: Copying task-specific data into a central accumulator. This is crucial for collecting results and metadata from individual tasks.
    *   **Serialization**: Translating the internal, granular task graph state into a structured, human-readable format suitable for APIs and user interfaces. This involves significant data re-organization and ensures a consistent external representation regardless of the underlying task execution details.

### Key Components and Responsibilities

#### Filters (`__init__.py`)

Filters are simple callables that take task, event, and accumulator arguments, and return a boolean. They are used primarily as predicates for `FilteringEvaluator` instances.

*   `Not`, `All`, `Any`: These are logical combinators for other filters, allowing complex conditions to be built (e.g., `All(TaskTypeEq('foo'), Not(TaskStatusIn(['pending'])))`).
*   `TaskTypeEq`, `TaskStatusIn`, `TaskIsEventTarget`: Specific filters that check properties of the `task` or `event` objects. These provide basic building blocks for common conditional checks.

#### Core Evaluator Combinators (`__init__.py`)

These classes define how evaluation logic is structured and executed. They take `task`, `event`, and `accumulator` arguments and return an iterable of actions or `None`.

*   `NoopEvaluator`: A placeholder evaluator that performs no actions. It's useful as a default or when no action is needed in a specific branch of logic.
*   `TaskPayloadLiftingEvaluator`: Responsible for extracting the payload and status from a `task` and copying them into the shared `accumulator` dictionary, keyed by the task's ID. This is a fundamental component for aggregating task-specific data for later processing or display. It includes options to selectively include or exclude keys or event types.
*   `SequenceEvaluator`: Executes a list of evaluators in a defined order. It collects and flattens all actions returned by its constituent evaluators. This is the primary way to define a series of steps for a given evaluation path.

    ```
    Event -> Evaluator A -> Evaluator B -> Evaluator C
               |              |              |
               v              v              v
           Actions A      Actions B      Actions C
           (flattened and returned)
    ```

*   `FilteringEvaluator`: Introduces conditional branching. It takes a `predicate` (a filter instance) and two other evaluators: a `delegate` and an `alternative`. If the predicate returns `True`, the `delegate` is executed; otherwise, the `alternative` (or `NoopEvaluator` if none is provided) is run. This allows for `if/else` logic within the evaluation flow.

    ```
                Is Condition True?
                     /    \
                   Yes    No
                  /        \
           Delegate Evaluator  Alternative Evaluator
    ```

*   `DispatchEvaluatorBase` (and its concrete subclasses: `DispatchByEventTypeEvaluator`, `DispatchByTaskStatus`, `DispatchByTaskType`): These evaluators dynamically select and execute a specific evaluator from a map, based on a key derived from the `task` or `event`. For example, `DispatchByTaskType` selects an evaluator based on `task.task_type`. This is crucial for routing tasks of different types to their specialized processing logic.
*   `Selector`: A specialized `FilteringEvaluator` that combines common filtering criteria (task type, event type, or a custom predicate) with `TaskPayloadLiftingEvaluator` as its delegate. It's a convenience class for lifting payload from tasks matching specific criteria.

#### Job Serialization (`job_serializer.py`)

This module provides the `Serializer` class, which is a specialized evaluator designed to transform the internal graph of Pinpoint tasks and their states into a structured dictionary for external consumption (e.g., the Pinpoint UI).

*   `Serializer`: Extends `DispatchByTaskType`. Its core responsibility is to orchestrate the conversion of internal task data into the canonical output schema. It maps different task types (e.g., `find_isolate`, `run_test`, `find_culprit`) to sequences of specific serializers (e.g., `find_isolate.Serializer()`) followed by generic transformers (`TaskTransformer`, `AnalysisTransformer`).

    The `Serializer` employs a specific protocol for merging a `local_context` (produced by task-specific serializers and transformers) into a `global context`. This involves:
    *   Identifying and updating entries in the `state` list (which represents individual changes/revisions in the job).
    *   Adding `quests` (legacy term for task types) to a global list.
    *   Aggregating execution details and result values for each state.
    *   Ordering states according to the `order_changes` information provided by analysis tasks.
    *   Setting global job parameters like `comparison_mode` and `metric`.

*   `TaskTransformer`: This function takes the raw payload lifted from an individual task (e.g., by `TaskPayloadLiftingEvaluator`) and transforms it into a standard `state` object. This state includes the `change` the task relates to, its `quest` (e.g., 'Build', 'Test'), the `index` (for multiple attempts/executions), and `add_execution` details. This ensures consistent formatting for individual task results regardless of their original payload structure.

    ```
    TaskPayloadLiftingEvaluator Result:
    {
      'task_id': {
        'status': 'COMPLETED',
        'details': [{'key': 'url', 'value': 'http://example.com'}]
      }
    }
    |
    v
    TaskTransformer Output:
    {
      'state': {
        'change': {...},
        'quest': 'Build',
        'index': 0,
        'add_execution': {
          'status': 'COMPLETED',
          'details': [{'key': 'url', 'value': 'http://example.com'}]
        }
      }
    }
    ```

*   `AnalysisTransformer`: This function specifically processes the output from "find\_culprit" tasks (e.g., a performance bisection result). It extracts high-level job parameters like `comparison_mode` and `metric`, and provides the ordered list of `changes`, `comparisons`, and `result_values` that define the overall job's outcome. This is critical for structuring the final job summary.

    ```
    FindCulprit Task Result:
    {
      'task_id': {
        'comparison_mode': 'performance',
        'metric': 'total_duration',
        'changes': [{...}, {...}],
        'comparisons': [{...}, {...}],
        'result_values': [[1.0], [2.0]]
      }
    }
    |
    v
    AnalysisTransformer Output:
    {
      'set_parameters': {
        'comparison_mode': 'performance',
        'metric': 'total_duration'
      },
      'order_changes': {
        'changes': [{...}, {...}],
        'comparisons': [{...}, {...}],
        'result_values': [[1.0], [2.0]]
      }
    }
    ```

### Key Workflow: Job Serialization

The serialization process demonstrates how filters and evaluators combine to transform a complex, internal task graph into a simplified, user-facing representation.

```
+----------------+       +------------------------------------+
| Incoming Task  | ----> | job_serializer.Serializer.__call__ |
|  & Event       |       +------------------------------------+
+----------------+                 | (Delegates based on Task Type)
                                   |
                                   v
+--------------------------+  +-------------------------------+
| SequenceEvaluator for    |  | SequenceEvaluator for         |
| 'find_isolate', 'run_test',|  | 'find_culprit' tasks          |
| 'read_value' tasks       |  |                               |
+--------------------------+  +-------------------------------+
       |                      |
       v                      v
+------------------------+  +------------------------+
| Task-specific Serializer|  | performance_bisection. |
| (e.g., find_isolate.S) |  | Serializer             |
+------------------------+  +------------------------+
       |                      |
       v                      v
+------------------------+  +------------------------+
| TaskPayloadLiftingEvaluator |  | AnalysisTransformer   |
+------------------------+  +------------------------+
       |                      |
       v                      v
+--------------------------+  +--------------------------+
|  TaskTransformer         |  | Local Context for Culprit|
| (Local Context for Tasks)|  |  Data                    |
+--------------------------+  +--------------------------+
       |                      |
       |  (Returned to Serializer)
       +--------------------------------------------------+
                                   | (Serializer merges local context into global)
                                   v
+----------------------------------------------------------------------------------+
| Global Job Context (Accumulator)                                                 |
| (Updated with task states, ordered changes, comparisons, and job parameters)     |
+----------------------------------------------------------------------------------+
```


















# Module: /models/quest

The `/models/quest` module provides the foundational abstractions for defining and executing units of work within Pinpoint, specifically `Quests` and their `Executions`. This design enables Pinpoint to orchestrate complex operations, such as building Chromium, running performance tests, and collecting results for specific code changes, in a modular and fault-tolerant manner.

### Why and How

The module's architecture is driven by the need for a flexible and robust system to automate performance analysis:

*   **Decoupling Work Definition from Execution:** A `Quest` serves as a blueprint, defining a generic piece of work (e.g., "Build Chromium"). An `Execution`, on the other hand, represents a concrete, stateful instance of that work bound to a specific `Change` and its context. This separation allows the same `Quest` definition to be reused across different code changes or with varying parameters, while each `Execution` tracks its unique progress, inputs, and outputs.
*   **Structured Information Flow:** `Quest.Start()` acts as the entry point for initiating work, explicitly receiving a `Change` object and `result_arguments` from any preceding `Execution`. This ensures a clear, traceable flow of data (e.g., an isolate hash from a build step passed to a test step) between sequential tasks. For shared information across related `Executions` (e.g., a chosen bot ID for all tests in a try job), the `Quest` object itself can maintain and propagate this state, fostering coordination.
*   **Robust Error Handling:** The system distinguishes between two levels of errors in `Execution.Poll()`:
    *   **Job-level errors (inheriting from `StandardError`):** These indicate fundamental, unrecoverable problems (e.g., `ImportError`, `SyntaxError`) that should immediately fail the entire job, preventing wasted resources on subsequent steps.
    *   **Execution-level errors (any other `Exception`):** These signify issues specific to a particular task instance, allowing other `Executions` in the job to potentially proceed or enabling retry mechanisms for the failing `Execution` without collapsing the entire job. This distinction is critical for Pinpoint's resilience and efficiency.
*   **Modularity and Extensibility:** The `Quest` and `Execution` base classes define a clear interface for adding new types of work. Users can extend Pinpoint's capabilities by implementing these interfaces, integrating with custom infrastructure or build systems. The `FromDict` class method provides a standardized mechanism for creating `Quest` instances from declarative configurations (e.g., from a UI or API), further enhancing flexibility.

### Key Components

The module comprises two core abstract base classes and numerous concrete implementations:

1.  **`quest.py`**
    *   **Responsibility:** Defines the abstract interface for all types of work units in Pinpoint. It represents a single, self-contained step within a larger workflow.
    *   **Key Aspects:**
        *   `__str__()`: Provides a concise, human-readable name (e.g., "Build", "Test") for display in logs and the UI.
        *   `Start(self, change, *args)`: The primary method that, given a `Change` and optional `result_arguments` from a previous step, initiates and returns a new `Execution` object.
        *   `PropagateJob(self, job)`: Allows the `Quest` to gain context about the overall Pinpoint `Job` (e.g., `job_id`, `user`, `comparison_mode`). This information is crucial for decorating external tasks (like Swarming tasks) with relevant metadata for tracking and filtering.
        *   `FromDict(cls, arguments)`: A factory method that takes a dictionary of configuration arguments and constructs a `Quest` instance, enabling declarative definition of Pinpoint jobs.

2.  **`execution.py`**
    *   **Responsibility:** Represents a concrete instance of a `Quest`'s work, tied to a specific `Change` and managing its lifecycle, state, and outcomes.
    *   **Key Aspects:**
        *   **State Management:** Tracks internal flags such as `_completed`, `_exception`, `_result_values` (numerical performance metrics), and `_result_arguments` (structured data passed to the next `Execution`).
        *   `_AsDict()`: An abstract method that concrete `Execution` implementations use to provide detailed, debug-oriented information (e.g., links to Buildbucket, Swarming tasks, or Isolate/CAS artifacts) for UI display.
        *   `Poll()`: The method periodically invoked by Pinpoint's job runner to advance the `Execution`'s state. It includes robust error handling that distinguishes between fatal Job-level exceptions and Execution-level errors.
        *   `_Poll()`: An abstract method where the actual task-specific work of polling external services (e.g., checking build status, fetching Swarming results) or processing local data occurs.
        *   `_Complete(self, result_values=(), result_arguments=None)`: An internal helper method for `_Poll()` implementations to mark the execution as finished and store its results and arguments.

3.  **Specific Quest Implementations (e.g., `find_isolate.py`, `run_test.py`, `read_value.py`, and their derivatives)**
    *   These files implement concrete `Quest` and `Execution` pairs for specific types of work, extending the base `Quest` and `Execution` classes.
    *   **`find_isolate.py` (`FindIsolate` Quest and `_FindIsolateExecution`):**
        *   **Responsibility:** Manages the acquisition of a build artifact (an isolate hash or CAS reference) for a given `Change`. This often involves triggering a new build via Buildbucket or retrieving an existing one from a cache.
        *   **Key Aspects:** It encapsulates the logic for interacting with Buildbucket to request builds and monitor their status. It includes mechanisms for checking an isolate cache first, specifying fallback targets, and handling different build types (e.g., 'performance' vs. 'try' jobs). The `_FindIsolateExecution` tracks the specific build ID and its lifecycle, ultimately providing the `isolate_server`/`isolate_hash` or `cas_root_ref` as `result_arguments` to subsequent steps.
    *   **`run_test.py` (`RunTest` Quest and `_RunTestExecution`):**
        *   **Responsibility:** Serves as the base class for all quests that involve running commands or tests on Swarming. It handles the generic logic for creating Swarming tasks, polling their status, and interpreting results.
        *   **Key Aspects:** It constructs the Swarming task body, including dimensions, command, extra arguments, and tags. For 'try' jobs, it includes logic in `_StartAllTasks` to strategically assign tasks to bots with the least pending work. It monitors Swarming task states (`PENDING`, `RUNNING`, `COMPLETED`, `EXPIRED`, `FAILURE`) and reports Swarming job metrics (pending and running times). Subclasses specialize `_ComputeCommand` and `_ExtraTestArgs` to adapt to various testing frameworks.
    *   **`read_value.py` (`ReadValue` Quest and `ReadValueExecution`):**
        *   **Responsibility:** Extracts meaningful numerical results from test output files (e.g., `perf_results.json`) generated by benchmarks. It supports both legacy GraphJSON and modern HistogramSet data formats, and retrieval from either Isolate or CAS.
        *   **Key Aspects:** It orchestrates fetching the output file based on `isolate_server`/`isolate_hash` or `cas_root_ref`. It then attempts to parse the content, automatically detecting the data format. It extracts specific metric values by applying filters (metric name, grouping label, story, statistic) and handles scenarios where the desired data is missing, raising specific `ReadValue` errors. It also collects trace URLs for debugging.
    *   **Specialized `run_*.py` Quests:**
        *   **`run_browser_test.py`, `run_gtest.py`, `run_instrumentation_test.py`, `run_lacros_telemetry_test.py`, `run_performance_test.py`, `run_telemetry_test.py`, `run_vr_telemetry_test.py`, `run_web_engine_telemetry_test.py`, `run_webrtc_test.py`:** These modules provide specific implementations of `RunTest` tailored for different testing frameworks and environments within Chromium and related projects. They override methods like `_ComputeCommand` and `_ExtraTestArgs` to configure the precise command lines, working directories, and additional arguments required to execute tests like Telemetry benchmarks, GTests, Android instrumentation tests, and various platform-specific performance tests (Lacros, VR, WebEngine/Fuchsia, WebRTC). Each aims to abstract away the specific invocation details for its test type.

### Key Workflows

#### 1. General Pinpoint Job Flow (Quest and Execution Lifecycle)

This diagram illustrates the high-level progression of a typical Pinpoint job, showing how `Quests` initiate `Executions` and pass results between them.

```
                  Job Starts
                     |
                     v
          +--------------------+
          |   Quest 1 (Build)  |  (e.g., FindIsolate)
          +--------------------+
                | Start(change)
                v
          +--------------------+    Poll() calls _Poll() until complete
          | Execution 1 (Build)| <-------------------------------+
          | - Request Build    |                                 |
          | - Check Build Status |                               |
          +--------------------+                                 |
                | _Complete()                                   |
                | (result_arguments: isolate_hash or cas_root_ref)
                v                                                 |
          +--------------------+                                 |
          |   Quest 2 (Test)   |  (e.g., RunTelemetryTest)
          +--------------------+                                 |
                | Start(change, isolate_hash/cas_root_ref)         |
                v                                                 |
          +--------------------+                                 |
          | Execution 2 (Test) | <-------------------------------+
          | - Start Swarming Task|                               |
          | - Poll Swarming Status|                               |
          +--------------------+                                 |
                | _Complete()                                   |
                | (result_arguments: cas_root_ref or isolate_hash)
                v                                                 |
          +--------------------+                                 |
          |  Quest 3 (Read Value)| (e.g., ReadValue)
          +--------------------+                                 |
                | Start(change, cas_root_ref/isolate_hash)         |
                v                                                 |
          +--------------------+                                 |
          | Execution 3 (Read Value)| <---------------------------+
          | - Retrieve Output from CAS/Isolate |
          | - Parse Metrics        |
          +--------------------+
                | _Complete()
                | (result_values: [metric_value])
                v
                  Job Finished
```

#### 2. Error Handling in `Execution.Poll()`

This diagram highlights the robust error handling strategy implemented within the `Execution.Poll()` method.

```
      Execution.Poll()
            |
            v
      +---------------------------------+
      |        Try: self._Poll()        |
      |---------------------------------|
      |  If self._Poll() raises:        |
      |  - TokenRefreshError/DataStoreTimeoutError   ->  Raise errors.RecoverableError (Job-level)
      |  - errors.FatalError/RuntimeError          ->  Raise (Job-level)
      |  - Any other Exception (Execution-level)   ->  Catch, set self._completed = True,
      |                                                store exception for debugging.
      +---------------------------------+
            |
            v
      Finally: Ensure state is updated (e.g., logging).
```

#### 3. `FindIsolate` Quest Execution Workflow

This diagram details the internal logic of a `_FindIsolateExecution` as it progresses through its `Poll()` calls.

```
   _FindIsolateExecution.Poll()
         |
         v
   +-------------------------------------+
   | 1. Check Isolate Cache (with override)? | -- Found --> _Complete() -> END
   +-------------------------------------+
         | Not Found
         v
   +-------------------------------------+
   | 2. Check Isolate Cache (without override)?| -- Found --> _Complete() -> END
   +-------------------------------------+
         | Not Found
         v
   +-------------------------------------+
   | 3. Has a build been requested for    | -- Yes --> 4. Check Build Status
   |    this change (`self._build` set)? |
   +-------------------------------------+
         | No
         v
   +-------------------------------------+
   | 4. Request New Build                |  (Calls Buildbucket.Put(), stores build ID)
   |    - Stores build ID in `self._build` for subsequent polls.
   +-------------------------------------+
         |
         v (Next Poll() iteration will proceed to Check Build Status)
   +-------------------------------------+
   | 5. Check Build Status               |  (Calls Buildbucket.GetJobStatus())
   |    - If SCHEDULED/STARTED        ->  Continue Polling
   |    - If FAILURE/INFRA_FAILURE/CANCELED ->  Raise appropriate BuildError
   |    - If SUCCESS                  ->  1. Check Isolate Cache (again)
   +-------------------------------------+
```







































# Module: /models/tasks

This module defines the core tasks and their associated logic within Pinpoint, forming the building blocks for how performance analysis jobs are executed. At its heart, Pinpoint operates on a task graph, where each node represents a distinct operation, and dependencies define their execution order. This module provides the evaluators and graph construction logic for several key task types, orchestrating their lifecycle from initiation through completion or failure.

The module's design centers on an event-driven evaluation model. Task evaluators respond to events (like 'initiate' for a new task or 'update' for ongoing tasks) to process state changes, interact with external services, and dynamically adjust the task graph as needed. This asynchronous approach allows Pinpoint to efficiently manage complex, long-running operations and react to external system updates without constant polling.

The primary goal of this module is to ensure the robust execution and intelligent progression of performance analysis workflows, particularly bisections, by:
-   **Modularizing execution logic**: Each major step of a Pinpoint job (finding builds, running tests, reading results, bisecting changes) is encapsulated in a dedicated task type and its evaluator.
-   **Managing external service interactions**: Tasks are responsible for communicating with services like BuildBucket and Swarming, handling their specific APIs and response patterns.
-   **Dynamic workflow adaptation**: For complex processes like bisection, the system can dynamically extend the task graph based on intermediate results, refining its search space.
-   **Providing detailed state and outcomes**: Tasks meticulously record their status, results, and any encountered errors, enabling transparent tracking of job progression.

---

### Execution Engine

The central orchestrator for task graph evaluation. It consolidates the specific evaluators for each known task type into a unified pipeline.

**Why it exists:** To provide a single, consistent entry point for evaluating any task graph supported by Pinpoint. This ensures that all tasks are processed through a standardized mechanism, benefiting from common pre- and post-processing steps. It promotes modularity by allowing individual task evaluators to focus solely on their domain-specific logic, while the `ExecutionEngine` handles the overall flow.

**How it works:**
It's implemented as a `SequenceEvaluator` that first dispatches tasks to their specific type-based evaluators and then applies a common `TaskPayloadLiftingEvaluator`.

`evaluator.py` is responsible for its creation and setup:

```
Task Execution Flow:

Event -> (ExecutionEngine)
          |
          +-> DispatchByTaskType: Routes task to specific evaluator (e.g., find_isolate, run_test, read_value, find_culprit)
          |     |
          |     +-> [Task-specific Evaluator: Processes task, potentially schedules actions or updates payload]
          |
          +-> TaskPayloadLiftingEvaluator: Cleans up task payload (e.g., removes large, unnecessary data like Swarming request bodies)
          |
          V
        (Updated Task State)
```

**Key Components:**
-   **`ExecutionEngine` (in `evaluator.py`)**: A `SequenceEvaluator` that contains:
    -   `evaluators.DispatchByTaskType`: This crucial component routes a given task to the appropriate evaluator based on its `task_type`. This is how different task types (like `find_isolate` or `read_value`) get handled by their respective specialized logic.
    -   `evaluators.TaskPayloadLiftingEvaluator`: After a task's specific evaluation, this component standardizes the task's payload by "lifting" relevant data to the top level and removing large, transient data (e.g., `swarming_request_body`) that isn't needed for long-term state persistence or downstream processing. This helps keep the stored job state lean.

---

### Find Isolate Tasks

These tasks are responsible for obtaining a build artifact (an "isolate" or CAS root reference) for a specific code change, which is a prerequisite for running any tests.

**Why they exist:** Before any performance tests can be executed on a particular commit, a runnable build of that commit must be available. This task type handles the entire process of sourcing such a build, whether from a cache of previously built artifacts or by initiating a new build. This isolates the complexities of the build system from the test execution logic.

**How they work:**
An `initiate` event on a `find_isolate` task first attempts to locate a pre-existing isolate for the given change in a cache. If found, the task completes immediately. If not, it schedules a new build via BuildBucket. Subsequent `update` events, triggered by BuildBucket PubSub notifications or polling, track the build's progress. Upon build completion, the task extracts the isolate or CAS root reference and marks itself as completed or failed based on the build outcome.

`find_isolate.py` contains the logic for this task type.

```
Find Isolate Workflow:

[initiate event]
       |
       V
(InitiateEvaluator)
       |
       +-> [Check Isolate Cache]
             |
             +-> [Isolate Found?] --- Yes --> [Update Task Payload (isolate info)] --> [Task completed]
             |
             +-> No -------------------------> [ScheduleBuildAction]
                                                   |
                                                   V
                                                 BuildBucket
                                                   |
[update event] <--------------------------------- PubSub / Polling
       |
       V
(UpdateEvaluator)
       |
       +-> [UpdateBuildStatusAction]
             |
             +-> [Get Build Status from BuildBucket]
                   |
                   +-> [Build Completed Successfully?] --- Yes --> [Extract Isolate Hash/CAS Ref] --> [Task completed]
                   |
                   +-> No ---------------------------------------> [Task failed / cancelled]
```

**Key Components:**
-   **`InitiateEvaluator`**: This evaluator attempts to find an existing isolate for the specified change in the Pinpoint cache. If a cached isolate is found, the task is marked `completed`. Otherwise, it schedules a new build request with BuildBucket using the `ScheduleBuildAction`.
-   **`ScheduleBuildAction`**: An action that makes a request to the BuildBucket service to schedule a build. It updates the task payload with the build request details and sets the task status to `ongoing`.
-   **`UpdateEvaluator`**: This evaluator processes `update` events related to ongoing builds. It calls the `UpdateBuildStatusAction` to query BuildBucket for the latest build status (either from the event payload or by polling).
-   **`UpdateBuildStatusAction`**: This action parses the BuildBucket response. It extracts critical information like the isolate server and hash (or CAS root reference) if the build was successful. It handles various build failures (e.g., `FAILURE`, `CANCELLED` status, missing result details or properties) by marking the task `failed`.
-   **`TaskOptions`**: A `namedtuple` used to configure the parameters for creating a `find_isolate` task (e.g., builder, target, bucket, change).
-   **`Serializer`**: Formats the `find_isolate` task results for display, including links to the BuildBucket build page and the Isolate server.

---

### Run Test Tasks

These tasks are responsible for executing a performance test on a Swarming bot using a given build artifact.

**Why they exist:** Once a build artifact (isolate or CAS root) is available for a specific change, the next logical step is to run the actual performance test. This task type encapsulates the entire process of scheduling a test on Swarming, monitoring its execution, and retrieving its output. It bridges the gap between getting a build and getting raw test results.

**How they work:**
An `initiate` event for a `run_test` task first checks its `find_isolate` dependency. If the dependency is completed and provides isolate information, the `run_test` task schedules a Swarming job via `ScheduleTestAction`. Subsequent `update` events, usually from Swarming PubSub notifications, are handled by `PollSwarmingTaskAction`, which retrieves the Swarming task result, extracts the output isolate/CAS root reference, and marks the task as `completed` or `failed`.

`run_test.py` contains the logic for this task type.

```
Run Test Workflow:

[initiate event]
       |
       V
(InitiateEvaluator)
       |
       +-> [Check find_isolate dependency status]
             |
             +-> [Dependency Failed] ---------------------------------> [MarkTaskFailedAction] --> [Task failed]
             |
             +-> [Dependency Completed (isolate available)] -----------> [ScheduleTestAction]
                                                                             |
                                                                             V
                                                                          Swarming
                                                                             |
[update event] <---------------------------------------------------------- PubSub / Polling
       |
       V
(UpdateEvaluator)
       |
       +-> [PollSwarmingTaskAction]
             |
             +-> [Get Swarming Task Result]
                   |
                   +-> [Swarming Task Completed Successfully?] --- Yes --> [Extract Output Isolate/CAS Ref] --> [Task completed]
                   |
                   +-> No (e.g., EXPIRED, FAILURE) ---------------------> [Task failed]
```

**Key Components:**
-   **`InitiateEvaluator`**: This evaluator runs when a `run_test` task is initiated. It checks the status of its `find_isolate` dependency. If the isolate is successfully found, it creates a Swarming request body with the isolate details, test dimensions, and extra arguments, then schedules the Swarming task using `ScheduleTestAction`. If the dependency failed, it marks the `run_test` task as `failed`.
-   **`ScheduleTestAction`**: This action calls the Swarming API to schedule a new test execution. It sets up PubSub notifications for status updates and updates the task payload with the `swarming_task_id`.
-   **`UpdateEvaluator`**: This evaluator handles `update` events, typically triggered by Swarming PubSub messages. It instantiates `PollSwarmingTaskAction` to retrieve the latest status of the Swarming task.
-   **`PollSwarmingTaskAction`**: This action queries Swarming for the detailed result of the running test. It extracts the output isolate/CAS root reference and updates the `run_test` task's status based on the Swarming task's final state (`COMPLETED`, `EXPIRED`, `FAILED`).
-   **`TaskOptions`**: A `namedtuple` for configuring parameters to create `run_test` tasks (e.g., `swarming_server`, `dimensions`, `extra_args`, `attempts`).
-   **`Serializer`**: Formats the `run_test` task results for display, including links to the Swarming bot, task, and output isolate.
-   **`Validator`**: Provides pre-execution validation for `run_test` tasks, checking for missing dependencies or required payload keys.

---

### Read Value Tasks

These tasks are responsible for retrieving and parsing specific performance metric values from the raw output files of completed test runs.

**Why they exist:** After a `run_test` task completes, it provides an output isolate or CAS root reference containing test results (e.g., `perf_results.json`). This raw data needs to be processed to extract the precise metric value that Pinpoint is interested in (e.g., a specific histogram statistic or a value from a GraphJSON trace). This task type abstracts away the complexities of file retrieval and parsing, providing a structured value for further analysis.

**How they work:**
A `read_value` task depends on a `run_test` task. Once its `run_test` dependency is `completed` and provides an output isolate/CAS root, the `ReadValueEvaluator` retrieves the specified result file. It then parses this file, either as `histogram_sets` or `graph_json`, according to the task's configuration. It extracts the relevant value(s) and updates the task payload with the `result_values` before marking the task `completed`. Errors during retrieval or parsing lead to the task being `failed`.

`read_value.py` contains the logic for this task type.

```
Read Value Workflow:

[read_value task pending]
       |
       V
(Check run_test dependency) --- [run_test task completed (output isolate available)]
       |
       V
(ReadValueEvaluator)
       |
       +-> [RetrieveOutputJson (from Isolate/CAS)]
             |
             +-> [Parse data based on 'mode' (histogram_sets / graph_json)]
                   |
                   +-> [Extract specific metric values (e.g., histogram statistic, graph JSON trace value)]
                         |
                         V
                       [Update task payload with 'result_values'] --> [Task completed]
                         |
                         V
                       [Error (e.g., file not found, invalid JSON, metric not found)] --> [Task failed]
```

**Key Components:**
-   **`ReadValueEvaluator`**: This evaluator is triggered when its `run_test` dependency is completed. It retrieves the specified output JSON file from the isolate server or CAS, then dispatches to either `HandleHistogramSets` or `HandleGraphJson` based on the task's configured `mode`. It includes error handling for various issues like missing files, malformed JSON, or requested metrics not being found.
-   **`HandleHistogramSets`**: Parses result files containing `HistogramSet` data. It uses `histogram_helpers` to locate the target histogram by name, story, and grouping label, and then extracts the desired `statistic` (or raw samples if no statistic is specified). It also collects trace URLs if present in the histograms.
-   **`HandleGraphJson`**: Parses result files containing legacy GraphJSON data. It locates and extracts the value of a specific trace within a chart.
-   **`TaskOptions`**: A `namedtuple` for configuring parameters to create `read_value` tasks (e.g., `test_options`, `benchmark`, `histogram_options`, `graph_json_options`, `mode`).
-   **`HistogramOptions`, `GraphJsonOptions`**: Named tuples specifying how to locate specific data within HistogramSet or GraphJSON outputs, respectively.
-   **`Serializer`**: Formats the `read_value` task results, explicitly including `result_values` and any collected `trace_urls`.

---

### Performance Bisection Tasks

These tasks implement the core logic for automatically bisecting a range of commits to find the culprit for a performance change.

**Why they exist:** Manually identifying the single commit responsible for a performance regression or improvement in a large range of changes is time-consuming and prone to human error. This task type automates this process by intelligently running tests on intermediate commits, statistically comparing their results, and narrowing down the search space until the culprit commit is pinpointed or no significant change is found.

**How they work:**
A `find_culprit` task starts by first fetching the full range of commits between the specified `start_change` and `end_change` via `PrepareCommits`. It then initiates `read_value` tasks for the start and end commits (and potentially others in the range). As results from these `read_value` tasks become available, the `FindCulprit` evaluator performs statistical comparisons between adjacent changes. Based on these comparisons, it uses an exploration algorithm (`exploration.Speculate`) to decide which intermediate commits need testing next. If necessary, it will dynamically extend the task graph by adding new `read_value` tasks for these intermediate commits using `RefineExplorationAction`. This iterative process continues until a significant change is localized between two commits (a "culprit") or it's determined that no statistically significant change can be reproduced.

`performance_bisection.py` contains the logic for this task type.

```
Performance Bisection Workflow:

[initiate event]
       |
       V
(FindCulprit Evaluator)
       |
       +-> [PrepareCommits: Fetch all commits in the range start_change -> end_change]
             |
             +-> [Initially create read_value tasks for start_change and end_change]
                   |
                   V
(read_value tasks execute and complete) <-------------------------------------------------------------------------------------------------------------
       |                                                                                                                                              |
       V                                                                                                                                              |
[find_culprit task receives update event / re-evaluates]                                                                                               |
       |                                                                                                                                              |
       +-> [Gather results from all completed read_value dependencies]                                                                                  |
       |                                                                                                                                              |
       +-> [Statistical Comparison (Compare.Compare): Pairwise comparison of results between ordered changes]                                         |
       |                                                                                                                                              |
       +-> [Exploration (exploration.Speculate): Determine next steps]                                                                                |
             |                                                                                                                                        |
             +-> [Significant Change Detected (Culprit Found)?] --- Yes --> [Update payload with culprits] --> [Task completed]                        |
             |                                                                                                                                        |
             +-> [Inconclusive / Need More Data (Refinement Needed)?] ----> [RefineExplorationAction: Increase attempts for existing read_value tasks] --+
             |                                                                                                                                        |
             +-> [Intermediate Commit Needs Testing?] ---------------------> [RefineExplorationAction: Add new read_value tasks for intermediate commit]--+
             |                                                                                                                                        |
             +-> [No Statistically Significant Change / All Dependencies Failed] -> [Task failed / completed with no culprits]
```

**Key Components:**
-   **`CreateGraph`**: Generates the initial task graph for a bisection job. This typically includes a `find_culprit` task at the top level, with dependencies on `read_value` tasks for the `start_change` and `end_change` of the bisection range. These `read_value` tasks, in turn, depend on `run_test` and `find_isolate` tasks.
-   **`PrepareCommits`**: An action that fetches the complete list of commits between the start and end of the bisection range from Gitiles. This full list is stored in the `find_culprit` task's payload, enabling the bisection algorithm to operate on the entire history efficiently.
-   **`FindCulprit`**: This is the core evaluator for the bisection algorithm.
    -   It gathers results from all its `read_value` dependencies.
    -   It uses `compare.Compare` to perform statistical comparisons between performance values from different commits. This determines if a change between two commits is statistically `DIFFERENT`, `SAME`, or `UNKNOWN` (inconclusive).
    -   It employs `exploration.Speculate` to intelligently decide the next steps:
        -   If a statistically significant difference is found between two commits, these are identified as potential `culprits`.
        -   If comparisons are `UNKNOWN`, `RefineExplorationAction` is triggered to increase the number of attempts for the `read_value` tasks on those commits, to gather more data and increase confidence.
        -   If a broad range is still `UNKNOWN` or `SAME` but could contain a `DIFFERENT` section, `RefineExplorationAction` is triggered to add new `read_value` tasks for intermediate commits within that range, effectively narrowing down the search.
    -   Once the bisection process converges (either culprits are found or no significant change is reproducible), the task is marked `completed`.
-   **`RefineExplorationAction`**: This action dynamically extends the Pinpoint task graph by adding new `read_value` tasks or increasing the `attempts` for existing `read_value` tasks. This is fundamental to how the bisection algorithm adaptively explores the commit range.
-   **`ComputeExtraArgs`**: A helper function to dynamically generate additional arguments for the test runner based on the specific benchmark target (e.g., Telemetry, VR, WebRTC, GTest), ensuring the correct flags are passed to the underlying testing framework.
-   **`Serializer`**: Formats the bisection analysis results for display, including the identified `culprits`, statistical `comparisons`, and `result_values`.

---

### Bisection Test Utilities

This module provides a suite of helper functions and mock objects specifically designed to facilitate testing of the bisection and related task logic.

**Why they exist:** Testing complex, distributed workflows involving external services (BuildBucket, Swarming, Isolate) and dynamic graph modifications (bisection) can be challenging. This utility module offers controlled, predictable simulations of these components, enabling developers to write focused unit and integration tests for different scenarios without requiring actual service interactions. This accelerates development and improves the reliability of the core bisection algorithms.

**How they work:**
`bisection_test_util.py` offers a `BisectionTestBase` class that sets up a common test environment, including pseudo-changes and a function to populate a basic bisection graph. It then provides "fake" evaluator delegates that mimic the behavior of real `find_isolate`, `run_test`, and `read_value` tasks, allowing tests to precisely control outcomes such as successful isolate discovery, test failures, or specific metric results.

**Key Components:**
-   **`BisectionTestBase`**: A base class for bisection-related tests. It provides convenience properties for `start_change` and `end_change` using special pseudo-hashes (`commit_0`, `commit_5`) that are understood by the Pinpoint test infrastructure to represent a range of commits.
-   **`PopulateSimpleBisectionGraph`**: A helper that populates a job with a predefined, simple bisection task graph. This provides a consistent starting point for many bisection tests.
-   **`BisectionEvaluatorForTesting`**: Creates a specialized evaluator stack for testing bisection logic. It wraps the main `performance_bisection.Evaluator` with `FilteringEvaluator` and `TaskPayloadLiftingEvaluator`, and allows injecting custom "seeded" evaluators (the `Fake*` classes) for specific task types to control their outcomes during a test.
-   **`FakeReadValueSameResult`, `FakeReadValueFails`, `FakeReadValueMapResult`**: Mock evaluators for `read_value` tasks. They can be configured to always return the same result, always fail, or return results based on a predefined map of changes to values, respectively.
-   **`FakeNotFoundIsolate`, `FakeFoundIsolate`, `FakeFindIsolateFailed`**: Mock evaluators for `find_isolate` tasks. They simulate scenarios where an isolate is not found, successfully found, or where the build process fails.
-   **`FakeSuccessfulRunTest`, `FakeFailedRunTest`**: Mock evaluators for `run_test` tasks. They simulate successful test execution or test failures.
-   **`UpdateWrapper`**: A `namedtuple` used within test actions to cleanly encapsulate the arguments for `task_module.UpdateTask`, improving test readability and debugging.














# Module: /models/templates

This module centralizes the Jinja2 templates used by the Pinpoint system to generate various user-facing notifications, reports, and informational messages. The primary purpose of these templates is to transform complex bisection results and job statuses into clear, consumable, and actionable communications for users, typically delivered via email or displayed in a web interface.

The design philosophy behind encapsulating these messages as templates is to:
*   **Standardize Communication:** Ensure consistent formatting and messaging across different types of Pinpoint outputs, regardless of the underlying data.
*   **Decouple Content from Logic:** Separate the presentation layer from the application logic that performs performance analysis and job management. This allows for easier updates to message content or formatting without modifying core application code.
*   **Improve User Experience:** Provide users with immediate, structured information about their jobs or identified regressions, including direct links to relevant resources and guidance on next steps.

When Pinpoint completes a task, identifies a regression, or needs to inform a user about a job's status, it gathers relevant data (e.g., commit information, metric values, job parameters) and passes this context to one of these templates for rendering. The resulting formatted text or HTML is then used in the appropriate communication channel.

### Key Components and Responsibilities

*   **`differences_found.j2`**:
    This template is responsible for generating the primary notification when Pinpoint successfully identifies one or more significant performance differences (regressions or improvements) during a bisection. Its design focuses on clearly presenting the most impactful findings. It enumerates the top identified culprits, providing crucial details such as commit subject, author, URL, and the measured metric difference. Furthermore, it includes boilerplate text for understanding regressions and instructions for user actions if a culprit seems incorrect, aiming to guide users through the post-analysis process and potential triaging. The template is designed to be concise for single differences and expandable for multiple, highlighting the most relevant ones. It also handles the scenario where "no values" transitions were observed during the bisection, providing a brief summary of those.

*   **`job_created.j2`**:
    This template's role is to provide immediate feedback to a user upon the successful creation and queuing of a Pinpoint job. It serves as a confirmation and a quick reference, summarizing key job parameters like the benchmark, story, and measurement. Crucially, it provides a direct URL to the newly created job, allowing the user to monitor its progress. By displaying the current queue and pending job count, it offers transparency into the job's immediate status within the Pinpoint system.

*   **`missing_values.j2`**:
    This template addresses a specific scenario where a bisection run encounters revisions that produce "no values" for the measured metric. This could occur due to various reasons, such as benchmark failures, build failures, or configuration issues within the bisected range. Instead of simply reporting an error, this template is designed to diagnose and guide the user. It lists the revisions where values were missing and provides actionable advice on how to retry the bisection or debug the underlying problem (e.g., providing a patch for broken builds, reducing the bisection range). It also directs users to the Chromeperf-CulpritDetection-NeedsAttention hotlist for further assistance, ensuring users are not left without recourse when facing such issues.

### Workflow for Generating Notifications

The general workflow for generating a user notification or report involves the Pinpoint application's backend logic assembling the necessary data and selecting the appropriate template.

```
+-------------------+      +-----------------------+      +------------------+
| Pinpoint Backend  |      |   Data Context (e.g., |      | Rendered Output  |
|   (Analysis,      |----->|  Commit Info, Metric  |----->|   (Email body,   |
|   Job Management) |      |   Values, Job Args)   |      | Web UI element)  |
+-------------------+      +-----------------------+      +------------------+
          |                            ^
          |                            |
          |  Selects and loads         |
          |  the appropriate template  |
          +----------------------------+
                       |
                       V
        +---------------------------+
        | Jinja2 Template (e.g.,    |
        | differences_found.j2)     |
        +---------------------------+
```
This diagram illustrates how dynamic data from Pinpoint's backend is fed into a chosen Jinja2 template, which then processes this data to produce the final, formatted output for user consumption.








# Module: /static

The `/static` module centralizes front-end assets, core JavaScript utilities, and dedicated testing infrastructure. It is designed to provide reusable client-side functionalities that enhance user interaction and streamline the development and testing of web components.

### Client-Side Utilities

The primary purpose of the JavaScript within this module is to offer fundamental services for interactive elements.

**`autocomplete.html` (containing `autocomplete.js`)**
This file implements a robust fuzzy autocomplete mechanism, significantly improving user experience by allowing quick and forgiving searches through lists of items. The design prioritates both accuracy and responsiveness.

*   **Why fuzzy autocomplete:** Traditional exact-match search can be cumbersome. Fuzzy matching allows users to find items even with partial or slightly misspelled inputs, reducing cognitive load and improving discoverability in large datasets. It's crucial for dynamic user interfaces where rapid data filtering is expected.
*   **How it works:** The core is the `FuzzySelect` class, which takes a query and an accessor function (to extract the searchable string from an item). It splits the query into parts and checks if all parts are present in an item's searchable string. A simple scoring mechanism (sum of first `indexOf` positions for each query part) ranks matches, where a lower score indicates a better match (i.e., query parts appearing earlier in the string). The `Autocomplete` class then leverages `FuzzySelect`. It processes an initial `sourceList`, distinguishing "head items" (group headers) from regular items. When a search is performed, it filters regular items using `FuzzySelect`, identifies the groups these matched items belong to, and then includes the corresponding head items in the results. Finally, results are sorted first by group, then by whether they are a head item, then by their fuzzy score, and finally lexicographically by name, ensuring a logical and predictable presentation to the user.
*   **Responsibilities:** Provides fuzzy string matching, dynamic item filtering, and structured result sorting for autocomplete suggestions in various UI contexts.

### User Interface Assets

This module also hosts static assets crucial for visual branding and enhancing user interaction through clear iconography.

**`logo.png`, `logo.svg`**
These files provide the application's branding logo in both raster (PNG) and vector (SVG) formats.
*   **Why multiple formats:** PNG is suitable for broad compatibility and cases where raster images are preferred, while SVG ensures scalability without loss of quality, which is critical for responsive designs across various screen resolutions and zoom levels.
*   **Responsibilities:** Establish and maintain the visual identity of the application.

**`sort-down.svg`, `sort-up-down.svg`, `sort-up.svg`**
These SVG icons represent different states of sortable columns in data tables or lists.
*   **Why dedicated icons:** Clear visual cues are essential for intuitive user interfaces. These icons unambiguously communicate the current sorting direction (ascending, descending) or the possibility of sorting when a column is unsorted, guiding user interaction effectively.
*   **Responsibilities:** Provide standardized iconography for common sorting controls, improving the clarity and usability of interactive data displays.

### Testing Infrastructure

A significant part of this module is dedicated to supporting robust front-end testing.

**`testing_common.html` (containing `testing_common.js`)**
This file provides a suite of utilities primarily focused on mocking network requests and managing DOM elements for client-side unit and integration tests.

*   **Why mock network requests:** Testing components that rely on HTTP requests in isolation can be challenging. Actual network calls introduce flakiness, slowness, and external dependencies. Mocking `XMLHttpRequest` ensures tests are fast, deterministic, and self-contained, allowing developers to precisely control the data received by their components without actual server interaction.
*   **How it works:**
    *   **`XMLHttpRequest` Mocking:** It overrides the global `window.XMLHttpRequest` with a custom `MockXMLHttpRequest` class. Test cases use `addXhrMock(url, response)` to pre-define expected responses for specific URL patterns or all requests (`*`). When application code makes an `XMLHttpRequest`, the mock intercepts it, matches the request URL against its configured responses (prioritizing exact matches, then regex patterns, then a wildcard), and immediately provides the specified `responseText`, simulating a network response. `clearXhrMock()` restores the original `XMLHttpRequest` after tests.
    *   **Fixture Management:** `addToFixture(node)` and `clearFixture()` provide a controlled environment for testing UI components by appending them to a dedicated DOM element (`fixture`) and clearing it between tests. This prevents test pollution and ensures each component test starts with a clean slate.
    *   **URL Parameter Handling:** `sortQueryPart` and `paramString` normalize URL query parameters, which is crucial for consistently matching URLs in the XHR mock, regardless of the order in which parameters are serialized.
    *   **Specialized Mocking:** `mockChartJson` offers a higher-level abstraction for mocking common graph data queries, reducing boilerplate in tests involving chart components.
*   **Responsibilities:** Establishes a foundational framework for front-end testing by providing mechanisms for HTTP request interception, DOM manipulation, and data handling specific to test environments.

**Workflow for XHR Mocking:**

```
Test Setup:
[Test Script] ------------> addXhrMock(url, response)
      |                       |
      |                       V
      |                  [testing_common.js]
      |                       |  - Stores url-response mapping.
      |                       |  - Replaces window.XMLHttpRequest with MockXMLHttpRequest.
      |                       V

During Test Execution (Application Code under test):
[Application Component] --> new XMLHttpRequest()
      |                       |
      |                       V
      |                  [MockXMLHttpRequest]
      |                       |  - Intercepts open(method, url) and send(data).
      |                       |  - Looks up 'url + data' in stored mappings.
      |                       V
      |                  [testing_common.js]
      |                       |  - Retrieves mock response.
      |                       V
      |                  [MockXMLHttpRequest]
      |                       |  - Sets this.responseText.
      |                       |  - Calls this.onload() (simulating network completion).
      V                       |
[Application Component] <---- Mock Response Data (responseText)
      |                       |

Test Teardown:
[Test Script] ------------> clearXhrMock()
      |                       |
      |                       V
      |                  [testing_common.js]
      |                       |  - Restores original window.XMLHttpRequest.
      |                       V
```









